[
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "dill",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dill",
        "description": "dill",
        "detail": "dill",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "StringIO",
        "description": "StringIO",
        "detail": "StringIO",
        "documentation": {}
    },
    {
        "label": "pands",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pands",
        "description": "pands",
        "detail": "pands",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Object",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "Object2",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "Object3",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "lib1",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib2",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib3",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib4",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib5",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib6",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib7",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib8",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib9",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib10",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib11",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib12",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib13",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib14",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib15",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "google.protobuf.descriptor_pb2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.protobuf.descriptor_pb2",
        "description": "google.protobuf.descriptor_pb2",
        "detail": "google.protobuf.descriptor_pb2",
        "documentation": {}
    },
    {
        "label": "Bar",
        "importPath": "source",
        "description": "source",
        "isExtraImport": true,
        "detail": "source",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "pairwise",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "islice",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Pattern",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Pattern",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Pattern",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AnyStr",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Pattern",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "httpx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "httpx",
        "description": "httpx",
        "detail": "httpx",
        "documentation": {}
    },
    {
        "label": "gnupg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gnupg",
        "description": "gnupg",
        "detail": "gnupg",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "fnmatch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fnmatch",
        "description": "fnmatch",
        "detail": "fnmatch",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "git",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "git",
        "description": "git",
        "detail": "git",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "TestClient",
        "importPath": "fastapi.testclient",
        "description": "fastapi.testclient",
        "isExtraImport": true,
        "detail": "fastapi.testclient",
        "documentation": {}
    },
    {
        "label": "TestClient",
        "importPath": "fastapi.testclient",
        "description": "fastapi.testclient",
        "isExtraImport": true,
        "detail": "fastapi.testclient",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "isExtraImport": true,
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "CONTEXT_STORE",
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "isExtraImport": true,
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "isExtraImport": true,
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "call_echo_tool",
        "importPath": "src.mcp_tools.echo_tool.client",
        "description": "src.mcp_tools.echo_tool.client",
        "isExtraImport": true,
        "detail": "src.mcp_tools.echo_tool.client",
        "documentation": {}
    },
    {
        "label": "ParsedResource",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftInfo",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftType",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "AttributeDrift",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftInfo",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftType",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "AttributeDrift",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "ParsedResource",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "ParsedResource",
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "compare_states",
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "documentation": {}
    },
    {
        "label": "compare_attributes",
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IGNORED_ATTRIBUTES",
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "documentation": {}
    },
    {
        "label": "suggest_remediation",
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "documentation": {}
    },
    {
        "label": "parse_terraform_state_file",
        "importPath": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "description": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "documentation": {}
    },
    {
        "label": "parse_terraform_plan_json_file",
        "importPath": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "description": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "isExtraImport": true,
        "detail": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "documentation": {}
    },
    {
        "label": "PolicyConfig",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "BranchNamingPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "CommitMessagePolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "ConventionalCommitPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "RequireIssueNumberPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternsPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternItem",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "FileSizePolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG_FILENAME",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "BranchNamingPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "CommitMessagePolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "ConventionalCommitPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "RequireIssueNumberPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternsPolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternItem",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "FileSizePolicy",
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "branch",
        "importPath": "src.mcp_tools.pr_reviewer.policies",
        "description": "src.mcp_tools.pr_reviewer.policies",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.policies",
        "documentation": {}
    },
    {
        "label": "commit",
        "importPath": "src.mcp_tools.pr_reviewer.policies",
        "description": "src.mcp_tools.pr_reviewer.policies",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.policies",
        "documentation": {}
    },
    {
        "label": "file",
        "importPath": "src.mcp_tools.pr_reviewer.policies",
        "description": "src.mcp_tools.pr_reviewer.policies",
        "isExtraImport": true,
        "detail": "src.mcp_tools.pr_reviewer.policies",
        "documentation": {}
    },
    {
        "label": "run_command",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "description": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "peekOfCode": "def run_command(cmd):\n    \"\"\"\n    Execute a shell command and return its exit code, stdout, and stderr.\n    Args:\n        cmd: List of command arguments to execute\n    Returns:\n        Tuple containing (return_code, stdout, stderr)\n    \"\"\"\n    try:\n        process = subprocess.Popen(",
        "detail": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "documentation": {}
    },
    {
        "label": "update_cmd",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "description": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "peekOfCode": "update_cmd = [\"terraform-docs\", \".\"]\nreturn_code, stdout, stderr = run_command(update_cmd)\nif stderr:\n    print(f\"terraform-docs error: Warning during execution:\\n{stderr}\", file=sys.stderr)\n# Check git status for unstaged README changes\nstatus_cmd = [\"git\", \"status\", \"--porcelain\"]\nreturn_code, stdout, stderr = run_command(status_cmd)\n# Look for any README.md files in the unstaged changes\nunstaged_readmes = [\n    line.split()[-1]",
        "detail": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "documentation": {}
    },
    {
        "label": "status_cmd",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "description": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "peekOfCode": "status_cmd = [\"git\", \"status\", \"--porcelain\"]\nreturn_code, stdout, stderr = run_command(status_cmd)\n# Look for any README.md files in the unstaged changes\nunstaged_readmes = [\n    line.split()[-1]\n    for line in stdout.splitlines()\n    if line.startswith(\" M\") and line.endswith(\"README.md\")\n]\n# Check if we found any unstaged README files\nif len(unstaged_readmes) > 0:",
        "detail": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "documentation": {}
    },
    {
        "label": "unstaged_readmes",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "description": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "peekOfCode": "unstaged_readmes = [\n    line.split()[-1]\n    for line in stdout.splitlines()\n    if line.startswith(\" M\") and line.endswith(\"README.md\")\n]\n# Check if we found any unstaged README files\nif len(unstaged_readmes) > 0:\n    print(\"terraform-docs error: Please stage any README changes before committing.\")\n    sys.exit(1)\nprint(\"terraform-docs: Documentation is up to date\")",
        "detail": ".trunk.plugins.trunk.actions.terraform-docs.terraform-docs",
        "documentation": {}
    },
    {
        "label": "Example3",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "class Example3(   object ):\n    def __init__    ( self, bar ):\n     #Comments should have a space after the hash.\n     if bar : bar+=1;  bar=bar* bar   ; return bar\n     else:\n                    some_string = \"\"\"\n                       Indentation in multiline strings should not be touched.\nOnly actual code should be reindented.\n\"\"\"\n                    return (sys.path, some_string)",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "example1",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "def example1():\n    ####This is a long comment. This should be wrapped to fit within 72 characters.\n    some_tuple=(   1,2, 3,'a'  );\n    some_variable={'long':'Long code lines should be wrapped within 79 characters.',\n    'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],\n    'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,\n    20,300,40000,500000000,60000000000000000]}}\n    return (some_tuple, some_variable)\ndef example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\nclass Example3(   object ):",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "example2",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\nclass Example3(   object ):\n    def __init__    ( self, bar ):\n     #Comments should have a space after the hash.\n     if bar : bar+=1;  bar=bar* bar   ; return bar\n     else:\n                    some_string = \"\"\"\n                       Indentation in multiline strings should not be touched.\nOnly actual code should be reindented.\n\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "pick",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "peekOfCode": "pick = dill.dumps({\"a\": \"b\", \"c\": \"d\"})\nprint(dill.loads(pick))\nfile_obj = StringIO.StringIO()\ndill.dump([1, 2, \"3\"], file_obj)",
        "detail": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "file_obj",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "peekOfCode": "file_obj = StringIO.StringIO()\ndill.dump([1, 2, \"3\"], file_obj)",
        "detail": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"\ncachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\nimport pands\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "cachedir",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "cachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "description": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "description": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "peekOfCode": "def main(argv):\n    results = []\n    for line in sys.stdin.readlines():\n        filename, line_number, message = line.split(\":\")\n        results.append(\n            to_result_sarif(\n                filename, int(line_number), 0, \"misspelled\", message.strip()\n            )\n        )\n    sarif = {",
        "detail": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"\ncachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\nimport pands\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "cachedir",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "cachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\n# trunk-ignore(flake8/F401): this will trigger a warning to verify that the config is applied\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "peekOfCode": "aws_access_key_id = \"AKIAIO5FODNN7EXAMPLE\"\naws_token = \"AKIALALEMEL33243OLIA\"\nprivate_key = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\nQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XAAAAJDiKO3C4ijt\nwgAAAAtzc2gtZWQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XA\nAAAECzmj8DGxg5YHtBK4AmBttMXDQHsPAaCyYHQjJ4YujRBTxhYpjO266/FQgyiZzfO/Q5\n1cJPnsJzZxFi7dMxtf1cAAAADHJvb3RAZGV2aG9zdAE=\n-----END OPENSSH PRIVATE KEY-----\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "documentation": {}
    },
    {
        "label": "aws_token",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "peekOfCode": "aws_token = \"AKIALALEMEL33243OLIA\"\nprivate_key = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\nQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XAAAAJDiKO3C4ijt\nwgAAAAtzc2gtZWQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XA\nAAAECzmj8DGxg5YHtBK4AmBttMXDQHsPAaCyYHQjJ4YujRBTxhYpjO266/FQgyiZzfO/Q5\n1cJPnsJzZxFi7dMxtf1cAAAADHJvb3RAZGV2aG9zdAE=\n-----END OPENSSH PRIVATE KEY-----\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "documentation": {}
    },
    {
        "label": "private_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "peekOfCode": "private_key = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\nQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XAAAAJDiKO3C4ijt\nwgAAAAtzc2gtZWQyNTUxOQAAACA8YWKYztuuvxUIMomc3zv0OdXCT57Cc2cRYu3TMbX9XA\nAAAECzmj8DGxg5YHtBK4AmBttMXDQHsPAaCyYHQjJ4YujRBTxhYpjO266/FQgyiZzfO/Q5\n1cJPnsJzZxFi7dMxtf1cAAAADHJvb3RAZGV2aG9zdAE=\n-----END OPENSSH PRIVATE KEY-----\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "description": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "peekOfCode": "def to_result_sarif(path: str, lineno: int, colno: int, rule_id: str, message: str):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "description": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "peekOfCode": "def main(argv):\n    output_json = json.load(sys.stdin)\n    errors = output_json.get(\"errors\", [])\n    results = []\n    for error in errors:\n        rule = error.get(\"rule\", \"\")\n        message = error.get(\"message\", \"\")\n        location = error.get(\"location\")\n        if location:\n            path = location.get(\"file\", \"\")",
        "detail": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "documentation": {}
    },
    {
        "label": "try_find_string_in_file",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def try_find_string_in_file(filename, search_string):\n    with open(filename, \"r\") as f:\n        for i, line in enumerate(f):\n            index = line.find(search_string)\n            if index != -1:\n                return i + 1, index + 1\n    return 0, 0\ndef to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def main(argv):\n    parser = argparse.ArgumentParser(description=\"Parse output of markdown-link-check\")\n    parser.add_argument(\"--target\", dest=\"target\")\n    args = parser.parse_args()\n    results = []\n    # Line numbers are not reported out of the tool right now - so we regex parse the output to extract issue codes\n    for line in sys.stdin:\n        parse_reg = \"\\s*(\\[.*\\])\\s(.*).*Status:\\s*(\\d*)(.*)\"\n        filename = args.target\n        parse_result = re.fullmatch(parse_reg, line, flags=re.DOTALL)",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "greeting",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def greeting(name: str) -> str:\n    return \"Hello \" + name\ndef printer() -> None:\n    print(\"Hello\")\ngreeting(3)\ngreeting(b\"Alice\")\na = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "printer",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def printer() -> None:\n    print(\"Hello\")\ngreeting(3)\ngreeting(b\"Alice\")\na = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "bad_foo",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "a = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "Bar",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "peekOfCode": "class Bar:\n  a: int\n  b: int\ndef bad_function() -> int:\n  print(\"returns nothing\")",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "documentation": {}
    },
    {
        "label": "bad_function",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "peekOfCode": "def bad_function() -> int:\n  print(\"returns nothing\")",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.nancy.parse",
        "description": ".trunk.plugins.trunk.linters.nancy.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.nancy.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.nancy.parse",
        "description": ".trunk.plugins.trunk.linters.nancy.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    nancy_output = json.load(sys.stdin)\n    for vuln_entry in nancy_output.get(\"vulnerable\", []):\n        for vuln in vuln_entry.get(\"Vulnerabilities\", []):\n            results.append(\n                to_result_sarif(\n                    \".\",\n                    0,\n                    0,",
        "detail": ".trunk.plugins.trunk.linters.nancy.parse",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"database_specific\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    vuln_metadata = vuln[\"database_specific\"]\n    if \"severity\" not in vuln_metadata:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln_metadata[\"severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, lineno: int, vuln_id: str, description: str, severity: str\n):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "join_common_sets",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def join_common_sets(lst):\n    init_len = 0\n    final_len = 1\n    while init_len != final_len:\n        init_len = len(lst)\n        ret = []\n        for s in lst:\n            unique = True\n            for stored_set in ret:\n                if len(stored_set.intersection(s)) > 0:",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_preferred_alias",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def get_preferred_alias(aliases):\n    for rx in PREFERRED_ORDER:\n        found_aliases = sorted(alias for alias in aliases if re.match(rx, alias))\n        if len(found_aliases) > 0:\n            return found_aliases[0]\n    return sorted(aliases)[0]\ndef main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.\n        if sys.platform == \"win32\":",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.\n        if sys.platform == \"win32\":\n            filtered_stdin = \"\".join(i for i in sys.stdin.read() if ord(i) < 256)\n            osv_json = json.loads(filtered_stdin)\n        else:\n            osv_json = json.load(sys.stdin)\n    except json.decoder.JSONDecodeError as err:\n        if str(err) == \"Expecting value: line 1 column 1 (char 0)\":",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"database_specific\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    vuln_metadata = vuln[\"database_specific\"]\n    if \"severity\" not in vuln_metadata:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln_metadata[\"severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "PREFERRED_ORDER",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "PREFERRED_ORDER = [\"GHSA-.*\", \"CVE-.*\", \"PYSEC-.*\"]\ndef get_preferred_alias(aliases):\n    for rx in PREFERRED_ORDER:\n        found_aliases = sorted(alias for alias in aliases if re.match(rx, alias))\n        if len(found_aliases) > 0:\n            return found_aliases[0]\n    return sorted(aliases)[0]\ndef main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "description": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "peekOfCode": "def main():\n    phpstan_json = json.loads(sys.stdin.read())\n    results = []\n    for file_name in phpstan_json[\"files\"]:\n        file_result = phpstan_json[\"files\"][file_name]\n        for result in file_result[\"messages\"]:\n            result = {\n                # We do not have a ruleId\n                \"message\": {\n                    \"text\": result[\"message\"],",
        "detail": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "description": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, description: str, line: int = 0, column: int = 0):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "description": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "peekOfCode": "def main(argv):\n    if len(argv) < 2:\n        print(\"Usage: trivy_to_sarif.py <exit_code>)\")\n        sys.exit(1)\n    if argv[1] == \"0\":\n        results = []\n        sarif = {\n            \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n            \"version\": \"2.1.0\",\n            \"runs\": [{\"results\": results}],",
        "detail": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "documentation": {}
    },
    {
        "label": "shift",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "shift = 3\nchoice = raw_input(\"would you like to encode or decode?\")\nword = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "choice",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "choice = raw_input(\"would you like to encode or decode?\")\nword = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "word",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "word = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "letters",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "letters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "encoded",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "encoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "foo",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "peekOfCode": "def foo():\n    return \"bar\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\" # Mypy treats this as an error because `x` is implicitly declared as `int`\na = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error\nclass Color(Enum):\n    RED = 1",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Color",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class Color(Enum):\n    RED = 1\n    BLUE = 2\ndef is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\n    # mypy reports error: Missing return statement\ndef func(val: int | None):",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "wrong_type",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def wrong_type(x: int) -> str:\n    return x  # error: Incompatible return value type (got \"int\", expected \"str\")\nclass A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\" # Mypy treats this as an error because `x` is implicitly declared as `int`\na = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "is_red",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\n    # mypy reports error: Missing return statement\ndef func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "func",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)\n            print(val + 1)  # mypy produces a false positive error here\n        inner_2 = lambda: reveal_type(val) + 1\n        inner_1()\n        inner_2()",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error\nclass Color(Enum):",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "description": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "peekOfCode": "results = []\nfor result in json.load(sys.stdin)[\"generalDiagnostics\"]:\n    parse = {\n        \"level\": result[\"severity\"] if result[\"severity\"] != \"information\" else \"note\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": result[\"file\"],\n                    },",
        "detail": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "description": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "description": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "description": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    content_json = sys.stdin.read()\n    content = json.loads(content_json)\n    for file_content in content:\n        messages = file_content.get(\"messages\", [])\n        if messages:\n            for msg in messages:\n                results.append(\n                    to_result_sarif(",
        "detail": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.renovate.parse",
        "description": ".trunk.plugins.trunk.linters.renovate.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.renovate.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.renovate.parse",
        "description": ".trunk.plugins.trunk.linters.renovate.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    content = sys.stdin.read()\n    parse_reg = \"(.*WARN:.*could not be parsed)(.*)\"\n    error_section = content.find('\"errors\": [')\n    parse_result = re.fullmatch(parse_reg, content, flags=re.DOTALL)\n    if parse_result:\n        warn_section = parse_result.group(2)\n        json_content = \"{\" + warn_section + \"}\"\n        error_output = json.loads(json_content)",
        "detail": ".trunk.plugins.trunk.linters.renovate.parse",
        "documentation": {}
    },
    {
        "label": "map_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "def map_severity(severity):\n    if severity in [\"convention\", \"refactor\", \"info\"]:\n        return \"note\"\n    if severity in [\"warning\"]:\n        return \"warning\"\n    if severity in [\"error\", \"fatal\"]:\n        return \"error\"\n    return \"none\"\nresults = []\nfor file in json.load(sys.stdin)[\"files\"]:",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "results = []\nfor file in json.load(sys.stdin)[\"files\"]:\n    for offense in file[\"offenses\"]:\n        parse = {\n            \"level\": map_severity(offense[\"severity\"]),\n            \"locations\": [\n                {\n                    \"physicalLocation\": {\n                        \"artifactLocation\": {\n                            \"uri\": file[\"path\"],",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\n# trunk-ignore(ruff/F401)\nimport json\nimport sys\nclass NoDocstring(object):\n    def __init__(self, arg1):",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.syntax.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.syntax.in",
        "peekOfCode": "def f(): {",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.syntax.in",
        "documentation": {}
    },
    {
        "label": "get_region",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "def get_region(entry, column_offset=0):\n    location = entry[\"location\"]\n    region = {\n        \"startColumn\": location[\"column\"] + column_offset,\n        \"startLine\": location[\"row\"],\n    }\n    if \"end_location\" in entry:\n        end_location = entry[\"end_location\"]\n        region[\"endColumn\"] = end_location[\"column\"] + column_offset\n        region[\"endLine\"] = end_location[\"row\"]",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "results = []\ndef get_region(entry, column_offset=0):\n    location = entry[\"location\"]\n    region = {\n        \"startColumn\": location[\"column\"] + column_offset,\n        \"startLine\": location[\"row\"],\n    }\n    if \"end_location\" in entry:\n        end_location = entry[\"end_location\"]\n        region[\"endColumn\"] = end_location[\"column\"] + column_offset",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "ruff_column_index",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "ruff_column_index = 1\nif len(sys.argv) > 1:\n    ruff_column_index = int(sys.argv[1])\nfor result in json.load(sys.stdin):\n    # As of ruff v0.0.260, some autofixable diagnostics may appear redundantly\n    if \"location\" not in result:\n        continue\n    filepath = result[\"filename\"]\n    # Ruff will set code to null for syntax errors\n    rule_id = result[\"code\"] or \"E999\"",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "unvalidated_value",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "description": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "peekOfCode": "def unvalidated_value(request):\n    value = request.GET.get('something')\n    function = globals().get(value)\n    if function:\n        return function(request)",
        "detail": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "peekOfCode": "def test():\n  substitution = \"hello %s\" % test\n  my_list = List()\n  try:\n    pass\n  except Exception:\n    raise Exception(\"test\")",
        "detail": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "peekOfCode": "test = \"world\"\ndef test():\n  substitution = \"hello %s\" % test\n  my_list = List()\n  try:\n    pass\n  except Exception:\n    raise Exception(\"test\")",
        "detail": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str,\n    start_line_number: int,\n    start_column_number: int,\n    end_line_number: Optional[int],\n    end_column_number: Optional[int],\n    rule_id: str,\n    message: str,\n):\n    region = {",
        "detail": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "peekOfCode": "def main(argv):\n    sqlfluff_json = json.load(sys.stdin)\n    results = []\n    for result in sqlfluff_json:\n        filepath = result[\"filepath\"]\n        for violation in result[\"violations\"]:\n            # In sqlfluff 3.0.0, line_no/line_pos replaced with start_*/end_*\n            start_line_number = violation.get(\"start_line_no\", violation.get(\"line_no\"))\n            start_column_number = violation.get(\n                \"start_line_pos\", violation.get(\"line_pos\")",
        "detail": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "description": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "peekOfCode": "def main(argv):\n    input_sarif = json.load(sys.stdin)\n    # strip \"file:\" from the beginning of each value in the 'file' field in the 'location' object in sarif format\n    for run in input_sarif[\"runs\"]:\n        for result in run[\"results\"]:\n            for location in result[\"locations\"]:\n                location[\"physicalLocation\"][\"artifactLocation\"][\"uri\"] = location[\n                    \"physicalLocation\"\n                ][\"artifactLocation\"][\"uri\"][5:]\n    print(json.dumps(input_sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.tfsec.parse",
        "description": ".trunk.plugins.trunk.linters.tfsec.parse",
        "peekOfCode": "def main():\n    original_input = sys.stdin.read()\n    try:\n        index = original_input.index(\"{\")\n        print(original_input[index:])\n    except ValueError:\n        print(original_input)\nif __name__ == \"__main__\":\n    main()",
        "detail": ".trunk.plugins.trunk.linters.tfsec.parse",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "aws_access_key_id = \"AKIAXYZDQCEN4EXAMPLE\"\naws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "aws_secret_access_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "aws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "github_secret",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "github_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "basic_auth",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "basic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "priv_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "priv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5\nWAi47VMFTkDZYS/eCvG53q9UBHpCj7Qvb0vSkCZXBvBIhlw193F3PX4WvO1IXsMwvQ1D1X",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, vuln_id: str, description: str, line: int = 0):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    path = trivy_json[\"ArtifactName\"]\n    results = []\n    for result in trivy_json.get(\"Results\", []):\n        if \"Misconfigurations\" not in result:\n            continue\n        for vuln in result[\"Misconfigurations\"]:\n            vuln_id = vuln[\"ID\"]\n            message = vuln[\"Message\"]",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in secret:\n        return DEFAULT_SARIF_SEVERITY\n    severity = secret[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,\n        \"locations\": [",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    results = []\n    for result in trivy_json.get(\"Results\", []):\n        path = trivy_json[\"ArtifactName\"]\n        for secret in result.get(\"Secrets\", []):\n            code = secret[\"RuleID\"]\n            description = secret[\"Title\"]\n            lineno = secret.get(\"StartLine\", 0)\n            results.append(",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in secret:\n        return DEFAULT_SARIF_SEVERITY\n    severity = secret[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):\n    return {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    results = []\n    lockfiles = {}\n    for result in trivy_json.get(\"Results\", []):\n        for vuln in result.get(\"Vulnerabilities\", []):\n            pkg_name = vuln[\"PkgName\"]\n            path = trivy_json[\"ArtifactName\"]\n            vuln_id = vuln[\"VulnerabilityID\"]\n            description = vuln[\"Title\"]",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "aws_access_key_id = \"AKIAXYZDQCEN4EXAMPLE\"\naws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "aws_secret_access_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "aws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "github_secret",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "github_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "basic_auth",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "basic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "priv_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "priv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5\nWAi47VMFTkDZYS/eCvG53q9UBHpCj7Qvb0vSkCZXBvBIhlw193F3PX4WvO1IXsMwvQ1D1X",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, line_number: int, vuln_id: str, description: str):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def sliding_window(iterable, n):\n    # sliding_window('ABCDEFG', 4) --> ABCD BCDE CDEF DEFG\n    it = iter(iterable)\n    window = collections.deque(islice(it, n - 1), maxlen=n)\n    for x in it:\n        window.append(x)\n        yield tuple(window)\nsecret_lineno_cache = {}\nfile_cache = {}\ndef find_line_number(secret, path):",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "find_line_number",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):\n        # trufflehog can report the same secret multiple times\n        # if it truly appears multiple times, then we want to log different lines for each issue",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def main(argv):\n    results = []\n    for line in sys.stdin.readlines():\n        vuln_json = json.loads(line)\n        # trufflehog doesn't have vuln IDs\n        # this is the name of the detector that found the error (e.g. AWS, Github, PrivateKey)\n        vuln_id = vuln_json[\"DetectorName\"]\n        # There also isn't description of the error aside from the raw secret, the redacted secret,\n        # and the detector that found it.\n        #",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "secret_lineno_cache",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "secret_lineno_cache = {}\nfile_cache = {}\ndef find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "file_cache",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "file_cache = {}\ndef find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):\n        # trufflehog can report the same secret multiple times",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "class A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\"\na = A()\nreveal_type(a.x)\na.x = \"\"\na.x = 3.0\nclass A:",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "class A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z)\nclass Color(Enum):\n    RED = 1",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Color",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "class Color(Enum):\n    RED = 1\n    BLUE = 2\ndef is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\ndef func(val: int | None):\n    if val is not None:",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "wrong_type",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "def wrong_type(x: int) -> str:\n    return x  # error: Incompatible return value type (got \"int\", expected \"str\")\nclass A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\"\na = A()\nreveal_type(a.x)\na.x = \"\"",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "is_red",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "def is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\ndef func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)\n            print(val + 1)",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "func",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "def func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)\n            print(val + 1)\n        inner_2 = lambda: reveal_type(val) + 1\n        inner_1()\n        inner_2()",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "a = A()\nreveal_type(a.x)\na.x = \"\"\na.x = 3.0\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "a.x = \"\"\na.x = 3.0\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z)",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "peekOfCode": "a.x = 3.0\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z)\nclass Color(Enum):",
        "detail": ".trunk.plugins.trunk.linters.ty.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n#whitespace below vvv\n  #A malindented comment\nif __name__ == \"__main__\" :\n      a=4+1",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n#whitespace below vvv\n  #A malindented comment\nif __name__ == \"__main__\" :\n      a=4+1\n      b=( 2*7 )\n      c = [1,\n           2,",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\n# trunk-ignore(flake8/F401): this will trigger a warning to verify that the config is applied\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "CreateContextRequest",
        "kind": 6,
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "peekOfCode": "class CreateContextRequest(BaseModel):\n    context_id: str\n    # Add other MCP-specific fields for context creation as needed\n    # For example: metadata: Optional[Dict[str, Any]] = None\n    # initial_data: Optional[Dict[str, Any]] = None\nclass UpdateContextRequest(BaseModel):\n    # Define fields for updating a context according to MCP\n    # For example: new_data: Dict[str, Any]\n    pass\nclass GetContextResponse(BaseModel):",
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "UpdateContextRequest",
        "kind": 6,
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "peekOfCode": "class UpdateContextRequest(BaseModel):\n    # Define fields for updating a context according to MCP\n    # For example: new_data: Dict[str, Any]\n    pass\nclass GetContextResponse(BaseModel):\n    context_id: str\n    data: Dict[str, Any]\n    # Add other MCP-specific fields\n@app.post(\"/v1/contexts\", status_code=201)\nasync def create_context(request: CreateContextRequest):",
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "GetContextResponse",
        "kind": 6,
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "peekOfCode": "class GetContextResponse(BaseModel):\n    context_id: str\n    data: Dict[str, Any]\n    # Add other MCP-specific fields\n@app.post(\"/v1/contexts\", status_code=201)\nasync def create_context(request: CreateContextRequest):\n    \"\"\"\n    Create a new context.\n    This is a simplified placeholder. MCP might have more specific requirements.\n    \"\"\"",
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "EchoPayload",
        "kind": 6,
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "peekOfCode": "class EchoPayload(BaseModel):\n    message: str\n    context_id: Optional[str] = None # Example: tool might operate within a context\n@app.post(\"/v1/tools/echo\")\nasync def echo_tool_endpoint(payload: EchoPayload):\n    \"\"\"\n    Echoes back the received message.\n    Optionally, this could interact with a context if context_id is provided.\n    \"\"\"\n    # For now, a simple echo.",
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "src.mcp_server.main",
        "description": "src.mcp_server.main",
        "peekOfCode": "app = FastAPI(\n    title=\"Anthropic Model Context Protocol Server\",\n    description=\"A server implementing the Anthropic Model Context Protocol (MCP).\",\n    version=\"0.1.0\",\n)\n# In-memory store for contexts (for demonstration purposes)\n# In a real application, this would be a persistent database.\nCONTEXT_STORE: Dict[str, Dict[str, Any]] = {}\nclass CreateContextRequest(BaseModel):\n    context_id: str",
        "detail": "src.mcp_server.main",
        "documentation": {}
    },
    {
        "label": "call_echo_tool",
        "kind": 2,
        "importPath": "src.mcp_tools.echo_tool.client",
        "description": "src.mcp_tools.echo_tool.client",
        "peekOfCode": "def call_echo_tool(\n    server_url: str,\n    message: str,\n    context_id: str = None,\n    http_client: httpx.Client = None\n) -> dict:\n    \"\"\"\n    Calls the echo tool endpoint on the MCP server.\n    Args:\n        server_url: The base URL of the MCP server.",
        "detail": "src.mcp_tools.echo_tool.client",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.mcp_tools.echo_tool.client",
        "description": "src.mcp_tools.echo_tool.client",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"MCP Echo Tool Client\")\n    parser.add_argument(\"message\", type=str, help=\"The message to send to the echo tool.\")\n    parser.add_argument(\"--server-url\", type=str, default=DEFAULT_SERVER_URL,\n                        help=f\"The base URL of the MCP server (default: {DEFAULT_SERVER_URL}).\")\n    parser.add_argument(\"--context-id\", type=str, help=\"Optional context ID to associate with the echo request.\")\n    args = parser.parse_args()\n    try:\n        result = call_echo_tool(args.server_url, args.message, args.context_id)\n        print(\"Server response:\")",
        "detail": "src.mcp_tools.echo_tool.client",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SERVER_URL",
        "kind": 5,
        "importPath": "src.mcp_tools.echo_tool.client",
        "description": "src.mcp_tools.echo_tool.client",
        "peekOfCode": "DEFAULT_SERVER_URL = \"http://localhost:8000\"\ndef call_echo_tool(\n    server_url: str,\n    message: str,\n    context_id: str = None,\n    http_client: httpx.Client = None\n) -> dict:\n    \"\"\"\n    Calls the echo tool endpoint on the MCP server.\n    Args:",
        "detail": "src.mcp_tools.echo_tool.client",
        "documentation": {}
    },
    {
        "label": "generate_gpg_key",
        "kind": 2,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "def generate_gpg_key(gpg_home: str, name: str, email: str, expiry: str, passphrase: str = None) -> tuple[str, str, str]:\n    \"\"\"\n    Generates a new GPG key pair.\n    Args:\n        gpg_home: Path to the GPG home directory to use.\n        name: Real name for the GPG key.\n        email: Email for the GPG key.\n        expiry: Expiration date for the GPG key (e.g., 0 for no expiry, 1y, 7d).\n        passphrase: Optional passphrase for the key. If None, key will not be passphrase protected.\n    Returns:",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "add_gpg_key_to_github",
        "kind": 2,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "def add_gpg_key_to_github(public_key_armored: str, github_token: str) -> bool:\n    \"\"\"\n    Adds the GPG public key to the authenticated user's GitHub account.\n    Args:\n        public_key_armored: The GPG public key in ASCII-armored format.\n        github_token: GitHub Personal Access Token with 'write:gpg_key' scope.\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    headers = {",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Generate a GPG key and add it to GitHub.\")\n    parser.add_argument(\"--name\", required=True, help=\"Real name for the GPG key (e.g., 'Tyler Zervas Agent').\")\n    parser.add_argument(\"--email\", required=True, help=\"Email for the GPG key (e.g., 'tz-dev-agent@vectorwieght.com').\")\n    parser.add_argument(\"--expiry\", default=DEFAULT_KEY_EXPIRY,\n                        help=f\"Expiration for the GPG key (e.g., 0, 1y, 30d). Default: {DEFAULT_KEY_EXPIRY}.\")\n    parser.add_argument(\"--passphrase\", default=None, help=\"Optional passphrase for the GPG key. If not provided, key will have no passphrase (less secure).\")\n    parser.add_argument(\"--github-token\", help=\"GitHub Personal Access Token with 'write:gpg_key' scope. Can also be set via GITHUB_TOKEN env var.\")\n    parser.add_argument(\"--gpg-home\", default=None,\n                        help=f\"Custom GPG home directory. Default: ~/{DEFAULT_GPG_HOME_RELATIVE} (a new temporary one is used if this is not set).\")",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "DEFAULT_GPG_HOME_RELATIVE",
        "kind": 5,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "DEFAULT_GPG_HOME_RELATIVE = \".gnupg_mcp_tool\" # Relative to user's home directory\nDEFAULT_KEY_EXPIRY = \"7d\" # Default to 7 days for short-lived keys\nGITHUB_API_URL = \"https://api.github.com\"\ndef generate_gpg_key(gpg_home: str, name: str, email: str, expiry: str, passphrase: str = None) -> tuple[str, str, str]:\n    \"\"\"\n    Generates a new GPG key pair.\n    Args:\n        gpg_home: Path to the GPG home directory to use.\n        name: Real name for the GPG key.\n        email: Email for the GPG key.",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEY_EXPIRY",
        "kind": 5,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "DEFAULT_KEY_EXPIRY = \"7d\" # Default to 7 days for short-lived keys\nGITHUB_API_URL = \"https://api.github.com\"\ndef generate_gpg_key(gpg_home: str, name: str, email: str, expiry: str, passphrase: str = None) -> tuple[str, str, str]:\n    \"\"\"\n    Generates a new GPG key pair.\n    Args:\n        gpg_home: Path to the GPG home directory to use.\n        name: Real name for the GPG key.\n        email: Email for the GPG key.\n        expiry: Expiration date for the GPG key (e.g., 0 for no expiry, 1y, 7d).",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "GITHUB_API_URL",
        "kind": 5,
        "importPath": "src.mcp_tools.gpg_github_tool.key_manager",
        "description": "src.mcp_tools.gpg_github_tool.key_manager",
        "peekOfCode": "GITHUB_API_URL = \"https://api.github.com\"\ndef generate_gpg_key(gpg_home: str, name: str, email: str, expiry: str, passphrase: str = None) -> tuple[str, str, str]:\n    \"\"\"\n    Generates a new GPG key pair.\n    Args:\n        gpg_home: Path to the GPG home directory to use.\n        name: Real name for the GPG key.\n        email: Email for the GPG key.\n        expiry: Expiration date for the GPG key (e.g., 0 for no expiry, 1y, 7d).\n        passphrase: Optional passphrase for the key. If None, key will not be passphrase protected.",
        "detail": "src.mcp_tools.gpg_github_tool.key_manager",
        "documentation": {}
    },
    {
        "label": "MockActualStateConnector",
        "kind": 6,
        "importPath": "src.mcp_tools.iac_drift_detector.connectors.mock_connector",
        "description": "src.mcp_tools.iac_drift_detector.connectors.mock_connector",
        "peekOfCode": "class MockActualStateConnector:\n    \"\"\"\n    Simulates fetching actual infrastructure state.\n    \"\"\"\n    def __init__(self, mock_data: Optional[List[Dict[str, Any]]] = None):\n        \"\"\"\n        Initializes the mock connector.\n        Args:\n            mock_data: A list of dictionaries, where each dictionary represents\n                       an actual resource's properties. If None, default mock data is used.",
        "detail": "src.mcp_tools.iac_drift_detector.connectors.mock_connector",
        "documentation": {}
    },
    {
        "label": "compare_attributes",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "peekOfCode": "def compare_attributes(\n    iac_attrs: Dict[str, Any],\n    actual_attrs: Dict[str, Any],\n    resource_type: str, # For type-specific ignore rules\n    ignored_attributes_config: Optional[Dict[str, List[str]]] = None\n) -> List[AttributeDrift]:\n    \"\"\"\n    Compares attributes of two resource states (IaC vs Actual).\n    Args:\n        iac_attrs: Attributes from the IaC definition.",
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "documentation": {}
    },
    {
        "label": "compare_states",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "peekOfCode": "def compare_states(\n    iac_resources: List[ParsedResource],\n    actual_resources: List[ParsedResource],\n    ignored_attributes_config: Optional[Dict[str, List[str]]] = None\n) -> List[DriftInfo]:\n    \"\"\"\n    Compares the desired state (from IaC) with the actual state (from cloud).\n    Args:\n        iac_resources: List of resources defined in IaC.\n        actual_resources: List of resources found in the actual environment.",
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.drift_engine",
        "documentation": {}
    },
    {
        "label": "suggest_remediation",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "description": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "peekOfCode": "def suggest_remediation(drift_info: DriftInfo, iac_tool: str = \"terraform\") -> List[str]:\n    \"\"\"\n    Generates human-readable remediation suggestions for a given drift.\n    Args:\n        drift_info: A DriftInfo object describing the detected drift.\n        iac_tool: The IaC tool in use (e.g., \"terraform\", \"pulumi\"). This helps tailor\n                  suggestions to specific commands.\n    Returns:\n        A list of strings, where each string is a suggested remediation action or comment.\n    \"\"\"",
        "detail": "src.mcp_tools.iac_drift_detector.core_logic.remediation",
        "documentation": {}
    },
    {
        "label": "parse_terraform_state_file",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "description": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "peekOfCode": "def parse_terraform_state_file(file_path: str) -> List[ParsedResource]:\n    \"\"\"\n    Parses a Terraform state file (.tfstate) and extracts resources.\n    Args:\n        file_path: Path to the .tfstate file.\n    Returns:\n        A list of ParsedResource objects.\n        Returns an empty list if parsing fails or no resources are found.\n    \"\"\"\n    try:",
        "detail": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "documentation": {}
    },
    {
        "label": "parse_terraform_plan_json_file",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "description": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "peekOfCode": "def parse_terraform_plan_json_file(file_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Parses a Terraform plan file (JSON output from `terraform show -json <planfile>`)\n    and extracts planned changes. This is more about *changes* than current state.\n    For drift detection, we are usually more interested in the state file for \"desired state\".\n    However, a plan can show what *would* change if applied, which can indicate drift\n    if the plan is generated against an out-of-sync state.\n    This function will extract a list of resource changes for now.\n    A more sophisticated drift tool might use this to predict drift resolution.\n    Args:",
        "detail": "src.mcp_tools.iac_drift_detector.parsers.terraform_parser",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.mcp_tools.iac_drift_detector.cli",
        "description": "src.mcp_tools.iac_drift_detector.cli",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"IaC Drift Detection and Remediation Suggestion Tool.\")\n    parser.add_argument(\"--iac-type\", type=str, default=\"terraform\", choices=[\"terraform\"],\n                        help=\"Type of Infrastructure as Code tool being used (default: terraform).\")\n    # Terraform specific arguments\n    tf_group = parser.add_argument_group('Terraform Options')\n    tf_group.add_argument(\"--tf-state-file\", type=str,\n                          help=\"Path to the Terraform state file (.tfstate) for desired state.\")\n    # tf_group.add_argument(\"--tf-plan-file\", type=str, # For future use with plan-based drift\n    #                       help=\"Path to a Terraform plan JSON file (from 'terraform show -json plan.out').\")",
        "detail": "src.mcp_tools.iac_drift_detector.cli",
        "documentation": {}
    },
    {
        "label": "ParsedResource",
        "kind": 6,
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "peekOfCode": "class ParsedResource(BaseModel):\n    \"\"\"Standardized representation of an IaC resource or an actual cloud resource.\"\"\"\n    id: str # The primary identifier of the resource within its provider\n    type: str # e.g., \"aws_instance\", \"aws_s3_bucket\" (Terraform type or equivalent)\n    name: str # The logical name (from IaC) or a descriptive name (if actual)\n    provider_name: str # e.g., \"aws\", \"google\", \"mock\"\n    module: Optional[str] = None # Module path if from IaC\n    attributes: Dict[str, Any] = Field(default_factory=dict) # Key-value pairs of resource attributes\n    # Additional field to distinguish source, could be useful\n    # source: str # e.g., \"terraform_state\", \"actual_cloud\", \"terraform_plan\"",
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftType",
        "kind": 6,
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "peekOfCode": "class DriftType(str, Enum):\n    MISSING_IN_ACTUAL = \"missing_in_actual\"      # Defined in IaC, not found in actual state\n    UNMANAGED_IN_ACTUAL = \"unmanaged_in_actual\"  # Found in actual state, not defined in IaC\n    MODIFIED = \"modified\"                      # Resource exists in both, but attributes differ\n    # NO_DRIFT = \"no_drift\" # Could be used for explicit non-drift reporting\nclass AttributeDrift(BaseModel):\n    attribute_name: str\n    iac_value: Any\n    actual_value: Any\nclass DriftInfo(BaseModel):",
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "AttributeDrift",
        "kind": 6,
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "peekOfCode": "class AttributeDrift(BaseModel):\n    attribute_name: str\n    iac_value: Any\n    actual_value: Any\nclass DriftInfo(BaseModel):\n    drift_type: DriftType\n    resource_type: str\n    resource_name: str # Logical name from IaC if available, or a descriptive name\n    resource_id: Optional[str] = None # Actual ID if available\n    iac_resource: Optional[ParsedResource] = None # The resource as defined in IaC",
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "DriftInfo",
        "kind": 6,
        "importPath": "src.mcp_tools.iac_drift_detector.models",
        "description": "src.mcp_tools.iac_drift_detector.models",
        "peekOfCode": "class DriftInfo(BaseModel):\n    drift_type: DriftType\n    resource_type: str\n    resource_name: str # Logical name from IaC if available, or a descriptive name\n    resource_id: Optional[str] = None # Actual ID if available\n    iac_resource: Optional[ParsedResource] = None # The resource as defined in IaC\n    actual_resource: Optional[ParsedResource] = None # The resource as found in actual state\n    attribute_drifts: List[AttributeDrift] = Field(default_factory=list) # For MODIFIED type\n    message: Optional[str] = None # General message about the drift\n    # Custom validator or logic could generate the message based on other fields.",
        "detail": "src.mcp_tools.iac_drift_detector.models",
        "documentation": {}
    },
    {
        "label": "check_branch_name_policy",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.branch",
        "description": "src.mcp_tools.pr_reviewer.policies.branch",
        "peekOfCode": "def check_branch_name_policy(\n    branch_name: Optional[str],\n    policy: BranchNamingPolicy\n) -> List[str]:\n    \"\"\"\n    Checks if the branch name conforms to the configured pattern.\n    Args:\n        branch_name: The name of the branch to check. Can be None (e.g. detached HEAD).\n        policy: The BranchNamingPolicy configuration object.\n    Returns:",
        "detail": "src.mcp_tools.pr_reviewer.policies.branch",
        "documentation": {}
    },
    {
        "label": "check_conventional_commit_format",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.commit",
        "description": "src.mcp_tools.pr_reviewer.policies.commit",
        "peekOfCode": "def check_conventional_commit_format(\n    commit_subject: str, # The first line of the commit message\n    commit_sha: str, # For context in violation messages\n    policy: ConventionalCommitPolicy\n) -> List[str]:\n    \"\"\"\n    Checks if the commit subject line adheres to Conventional Commits format.\n    Example: feat(scope)!: broadcast errors\n    Args:\n        commit_subject: The first line of the commit message.",
        "detail": "src.mcp_tools.pr_reviewer.policies.commit",
        "documentation": {}
    },
    {
        "label": "check_commit_for_issue_number",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.commit",
        "description": "src.mcp_tools.pr_reviewer.policies.commit",
        "peekOfCode": "def check_commit_for_issue_number(\n    commit_message_body: str, # Full commit message body (excluding subject, or could be full message)\n    pr_title: Optional[str], # Placeholder for future use\n    pr_body: Optional[str],  # Placeholder for future use\n    commit_sha: str, # For context in violation messages\n    policy: RequireIssueNumberPolicy\n) -> List[str]:\n    \"\"\"\n    Checks if the commit message body (or future PR title/body) contains an issue number.\n    Args:",
        "detail": "src.mcp_tools.pr_reviewer.policies.commit",
        "documentation": {}
    },
    {
        "label": "check_commit_message_policies",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.commit",
        "description": "src.mcp_tools.pr_reviewer.policies.commit",
        "peekOfCode": "def check_commit_message_policies(\n    commit_details: Dict, # As returned by GitUtils.get_commit_details\n    policy: CommitMessagePolicy,\n    # pr_title: Optional[str] = None, # For future PR context\n    # pr_body: Optional[str] = None   # For future PR context\n) -> List[str]:\n    \"\"\"\n    Runs all configured commit message policies for a single commit.\n    \"\"\"\n    violations: List[str] = []",
        "detail": "src.mcp_tools.pr_reviewer.policies.commit",
        "documentation": {}
    },
    {
        "label": "CONVENTIONAL_COMMIT_REGEX",
        "kind": 5,
        "importPath": "src.mcp_tools.pr_reviewer.policies.commit",
        "description": "src.mcp_tools.pr_reviewer.policies.commit",
        "peekOfCode": "CONVENTIONAL_COMMIT_REGEX = re.compile(r\"^(?P<type>[a-zA-Z_]+)(?:\\((?P<scope>[^\\)]+)\\))?(?P<breaking>!)?: (?P<subject>.+)$\")\ndef check_conventional_commit_format(\n    commit_subject: str, # The first line of the commit message\n    commit_sha: str, # For context in violation messages\n    policy: ConventionalCommitPolicy\n) -> List[str]:\n    \"\"\"\n    Checks if the commit subject line adheres to Conventional Commits format.\n    Example: feat(scope)!: broadcast errors\n    Args:",
        "detail": "src.mcp_tools.pr_reviewer.policies.commit",
        "documentation": {}
    },
    {
        "label": "check_content_disallowed_patterns",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.file",
        "description": "src.mcp_tools.pr_reviewer.policies.file",
        "peekOfCode": "def check_content_disallowed_patterns(\n    filepath: str,\n    get_file_content: GetFileContentCallable, # Function to get content (e.g., from git_utils)\n    policy: DisallowedPatternsPolicy\n) -> List[str]:\n    \"\"\"\n    Checks file content for disallowed patterns.\n    Args:\n        filepath: Path of the file being checked.\n        get_file_content: A callable that takes filepath and returns its content as string or bytes.",
        "detail": "src.mcp_tools.pr_reviewer.policies.file",
        "documentation": {}
    },
    {
        "label": "check_file_size_policy",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.policies.file",
        "description": "src.mcp_tools.pr_reviewer.policies.file",
        "peekOfCode": "def check_file_size_policy(\n    filepath: str,\n    get_file_size: GetFileSizeCallable, # Function to get file size (e.g., from git_utils)\n    policy: FileSizePolicy\n) -> List[str]:\n    \"\"\"\n    Checks if the file size exceeds the configured maximum.\n    Args:\n        filepath: Path of the file being checked.\n        get_file_size: A callable that takes filepath and returns its size in bytes.",
        "detail": "src.mcp_tools.pr_reviewer.policies.file",
        "documentation": {}
    },
    {
        "label": "GetFileContentCallable",
        "kind": 5,
        "importPath": "src.mcp_tools.pr_reviewer.policies.file",
        "description": "src.mcp_tools.pr_reviewer.policies.file",
        "peekOfCode": "GetFileContentCallable = Callable[[str], Optional[AnyStr]] # Takes path, returns content or None\nGetFileSizeCallable = Callable[[str], Optional[int]] # Takes path, returns size or None\ndef check_content_disallowed_patterns(\n    filepath: str,\n    get_file_content: GetFileContentCallable, # Function to get content (e.g., from git_utils)\n    policy: DisallowedPatternsPolicy\n) -> List[str]:\n    \"\"\"\n    Checks file content for disallowed patterns.\n    Args:",
        "detail": "src.mcp_tools.pr_reviewer.policies.file",
        "documentation": {}
    },
    {
        "label": "GetFileSizeCallable",
        "kind": 5,
        "importPath": "src.mcp_tools.pr_reviewer.policies.file",
        "description": "src.mcp_tools.pr_reviewer.policies.file",
        "peekOfCode": "GetFileSizeCallable = Callable[[str], Optional[int]] # Takes path, returns size or None\ndef check_content_disallowed_patterns(\n    filepath: str,\n    get_file_content: GetFileContentCallable, # Function to get content (e.g., from git_utils)\n    policy: DisallowedPatternsPolicy\n) -> List[str]:\n    \"\"\"\n    Checks file content for disallowed patterns.\n    Args:\n        filepath: Path of the file being checked.",
        "detail": "src.mcp_tools.pr_reviewer.policies.file",
        "documentation": {}
    },
    {
        "label": "run_all_checks",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.cli",
        "description": "src.mcp_tools.pr_reviewer.cli",
        "peekOfCode": "def run_all_checks(config: PolicyConfig, git_utils: GitUtils, base_branch: str, head_branch: str) -> List[str]:\n    \"\"\"\n    Runs all configured policy checks.\n    Args:\n        config: The loaded PolicyConfig.\n        git_utils: An instance of GitUtils.\n        base_branch: The base branch for comparison (e.g., 'main').\n        head_branch: The head branch to check (e.g., current feature branch, 'HEAD').\n    Returns:\n        A list of all violation messages.",
        "detail": "src.mcp_tools.pr_reviewer.cli",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.cli",
        "description": "src.mcp_tools.pr_reviewer.cli",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"PR Policy Review Tool: Checks code changes against configured policies.\")\n    parser.add_argument(\"--base-branch\", default=\"main\",\n                        help=\"The base branch to compare against (e.g., main, develop). Default: 'main'.\")\n    parser.add_argument(\"--head-branch\", default=\"HEAD\",\n                        help=\"The head branch or revision to check (e.g., your feature branch, 'HEAD'). Default: 'HEAD'.\")\n    parser.add_argument(\"--config-file\", default=None,\n                        help=f\"Path to the policy configuration YAML file. Defaults to searching for '.pr-policy.yml'.\")\n    parser.add_argument(\"--repo-path\", default=None,\n                        help=\"Path to the Git repository. Defaults to current working directory.\")",
        "detail": "src.mcp_tools.pr_reviewer.cli",
        "documentation": {}
    },
    {
        "label": "BranchNamingPolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class BranchNamingPolicy(BaseModel):\n    pattern: Optional[str] = \"^(feature|fix|chore|docs|style|refactor|test)/[a-zA-Z0-9_.-]+$\"\n    enabled: bool = True\n    @field_validator('pattern', mode='before')\n    def compile_pattern_branch(cls, v):\n        if v is None: # Should hit default if not provided, so this might not be strictly needed unless explicitly set to None\n            # If it can be None after defaults, handle it. Pydantic v2 handles defaults before validators usually.\n            # For a default string pattern, this 'if v is None' might be less relevant.\n            # Let's assume it's possible it's passed as None in the YAML.\n            return None",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "ConventionalCommitPolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class ConventionalCommitPolicy(BaseModel):\n    enabled: bool = True\n    types: List[str] = Field(default_factory=lambda: [\"feat\", \"fix\", \"docs\", \"style\", \"refactor\", \"test\", \"chore\"])\nclass RequireIssueNumberPolicy(BaseModel):\n    pattern: Optional[str] = \"\\\\[[A-Z]+-[0-9]+\\\\]\" # Example: [PROJ-123]\n    in_commit_body: bool = True # Check commit message body\n    in_pr_title: bool = False # Placeholder for future PR title check\n    in_pr_body: bool = False  # Placeholder for future PR body check\n    enabled: bool = False\n    @field_validator('pattern', mode='before')",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "RequireIssueNumberPolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class RequireIssueNumberPolicy(BaseModel):\n    pattern: Optional[str] = \"\\\\[[A-Z]+-[0-9]+\\\\]\" # Example: [PROJ-123]\n    in_commit_body: bool = True # Check commit message body\n    in_pr_title: bool = False # Placeholder for future PR title check\n    in_pr_body: bool = False  # Placeholder for future PR body check\n    enabled: bool = False\n    @field_validator('pattern', mode='before')\n    def compile_pattern_issue(cls, v):\n        if v is None:\n            return None",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "CommitMessagePolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class CommitMessagePolicy(BaseModel):\n    conventional_commit: ConventionalCommitPolicy = Field(default_factory=ConventionalCommitPolicy)\n    require_issue_number: RequireIssueNumberPolicy = Field(default_factory=RequireIssueNumberPolicy)\n    enabled: bool = True\nclass DisallowedPatternItem(BaseModel):\n    pattern: str\n    message: Optional[str] = None\n    enabled: bool = True\n    @field_validator('pattern', mode='before')\n    def compile_pattern_disallowed(cls, v):",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternItem",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class DisallowedPatternItem(BaseModel):\n    pattern: str\n    message: Optional[str] = None\n    enabled: bool = True\n    @field_validator('pattern', mode='before')\n    def compile_pattern_disallowed(cls, v):\n        if v is None: # Pattern is not Optional here, so this check is for robustness if data is bad\n            raise ValueError(\"Pattern for DisallowedPatternItem cannot be None\")\n        try:\n            return re.compile(v)",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DisallowedPatternsPolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class DisallowedPatternsPolicy(BaseModel):\n    patterns: List[DisallowedPatternItem] = Field(default_factory=list)\n    enabled: bool = True\nclass FileSizePolicy(BaseModel):\n    max_bytes: int = 1048576  # 1MB\n    ignore_extensions: List[str] = Field(default_factory=list)\n    ignore_paths: List[str] = Field(default_factory=list) # Paths/patterns to ignore for file size checks\n    enabled: bool = True\n# --- Main Configuration Model ---\nclass PolicyConfig(BaseModel):",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "FileSizePolicy",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class FileSizePolicy(BaseModel):\n    max_bytes: int = 1048576  # 1MB\n    ignore_extensions: List[str] = Field(default_factory=list)\n    ignore_paths: List[str] = Field(default_factory=list) # Paths/patterns to ignore for file size checks\n    enabled: bool = True\n# --- Main Configuration Model ---\nclass PolicyConfig(BaseModel):\n    branch_naming: BranchNamingPolicy = Field(default_factory=BranchNamingPolicy)\n    commit_messages: CommitMessagePolicy = Field(default_factory=CommitMessagePolicy)\n    disallowed_patterns: DisallowedPatternsPolicy = Field(default_factory=DisallowedPatternsPolicy)",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "PolicyConfig",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "class PolicyConfig(BaseModel):\n    branch_naming: BranchNamingPolicy = Field(default_factory=BranchNamingPolicy)\n    commit_messages: CommitMessagePolicy = Field(default_factory=CommitMessagePolicy)\n    disallowed_patterns: DisallowedPatternsPolicy = Field(default_factory=DisallowedPatternsPolicy)\n    file_size: FileSizePolicy = Field(default_factory=FileSizePolicy)\n    # Future policies can be added here\n    # documentation_changes: Optional[dict] = None\n# --- Loading Function ---\ndef load_config(config_path: Optional[str] = None) -> PolicyConfig:\n    \"\"\"",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "def load_config(config_path: Optional[str] = None) -> PolicyConfig:\n    \"\"\"\n    Loads policy configuration from a YAML file.\n    If config_path is None, tries to load from '.pr-policy.yml' in the current or parent directories.\n    If no file is found, returns default configuration.\n    \"\"\"\n    if config_path:\n        if not os.path.exists(config_path):\n            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n    else:",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG_FILENAME",
        "kind": 5,
        "importPath": "src.mcp_tools.pr_reviewer.config",
        "description": "src.mcp_tools.pr_reviewer.config",
        "peekOfCode": "DEFAULT_CONFIG_FILENAME = \".pr-policy.yml\"\n# --- Policy Specific Models ---\nclass BranchNamingPolicy(BaseModel):\n    pattern: Optional[str] = \"^(feature|fix|chore|docs|style|refactor|test)/[a-zA-Z0-9_.-]+$\"\n    enabled: bool = True\n    @field_validator('pattern', mode='before')\n    def compile_pattern_branch(cls, v):\n        if v is None: # Should hit default if not provided, so this might not be strictly needed unless explicitly set to None\n            # If it can be None after defaults, handle it. Pydantic v2 handles defaults before validators usually.\n            # For a default string pattern, this 'if v is None' might be less relevant.",
        "detail": "src.mcp_tools.pr_reviewer.config",
        "documentation": {}
    },
    {
        "label": "GitRepoError",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.git_utils",
        "description": "src.mcp_tools.pr_reviewer.git_utils",
        "peekOfCode": "class GitRepoError(Exception):\n    \"\"\"Custom exception for Git repository errors.\"\"\"\n    pass\nclass GitUtils:\n    def __init__(self, repo_path: Optional[str] = None):\n        \"\"\"\n        Initializes GitUtils.\n        Args:\n            repo_path: Path to the Git repository. Defaults to the current working directory.\n        Raises:",
        "detail": "src.mcp_tools.pr_reviewer.git_utils",
        "documentation": {}
    },
    {
        "label": "GitUtils",
        "kind": 6,
        "importPath": "src.mcp_tools.pr_reviewer.git_utils",
        "description": "src.mcp_tools.pr_reviewer.git_utils",
        "peekOfCode": "class GitUtils:\n    def __init__(self, repo_path: Optional[str] = None):\n        \"\"\"\n        Initializes GitUtils.\n        Args:\n            repo_path: Path to the Git repository. Defaults to the current working directory.\n        Raises:\n            GitRepoError: If the path is not a valid Git repository.\n        \"\"\"\n        try:",
        "detail": "src.mcp_tools.pr_reviewer.git_utils",
        "documentation": {}
    },
    {
        "label": "temp_tfstate_file",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def temp_tfstate_file(tmp_path: Path) -> Path:\n    \"\"\"Fixture to create a temporary .tfstate file for testing.\"\"\"\n    file_path = tmp_path / \"test.tfstate\"\n    # Update instance_type in tfstate to ensure it's different from mock for a clear drift\n    # tfstate_content_for_test = SAMPLE_TFSTATE_CONTENT.copy()\n    # tfstate_content_for_test[\"resources\"][0][\"instances\"][0][\"attributes\"][\"instance_type\"] = \"t2.small\"\n    # No, let's rely on the tags difference from the default mock data.\n    # And the S3 ACL difference.\n    with open(file_path, 'w') as f:\n        json.dump(SAMPLE_TFSTATE_CONTENT, f)",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "run_iac_cli",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def run_iac_cli(repo_path: Path, args: list[str]) -> subprocess.CompletedProcess:\n    \"\"\"Helper function to run the IaC Drift Detector CLI tool.\"\"\"\n    cmd = [\"python\", \"-m\", CLI_MODULE_PATH] + args\n    return subprocess.run(cmd, capture_output=True, text=True, cwd=repo_path) # Run from tmp_path itself or a sub-repo if needed\n# --- CLI Tests ---\ndef test_cli_help_message_iac():\n    \"\"\"Test if the IaC Drift CLI shows a help message.\"\"\"\n    result = subprocess.run([\"python\", \"-m\", CLI_MODULE_PATH, \"--help\"], capture_output=True, text=True)\n    assert \"usage: cli.py\" in result.stdout\n    assert \"--tf-state-file\" in result.stdout",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_help_message_iac",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def test_cli_help_message_iac():\n    \"\"\"Test if the IaC Drift CLI shows a help message.\"\"\"\n    result = subprocess.run([\"python\", \"-m\", CLI_MODULE_PATH, \"--help\"], capture_output=True, text=True)\n    assert \"usage: cli.py\" in result.stdout\n    assert \"--tf-state-file\" in result.stdout\n    assert \"--actual-state-source\" in result.stdout\n    assert result.returncode == 0\ndef test_cli_no_tfstate_file_error(tmp_path: Path):\n    \"\"\"Test CLI exits with error if --tf-state-file is required but not provided or not found.\"\"\"\n    result = run_iac_cli(tmp_path, [\"--iac-type\", \"terraform\"]) # No --tf-state-file",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_no_tfstate_file_error",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def test_cli_no_tfstate_file_error(tmp_path: Path):\n    \"\"\"Test CLI exits with error if --tf-state-file is required but not provided or not found.\"\"\"\n    result = run_iac_cli(tmp_path, [\"--iac-type\", \"terraform\"]) # No --tf-state-file\n    # print(\"STDOUT:\", result.stdout)\n    # print(\"STDERR:\", result.stderr)\n    assert result.returncode != 0 # Should be 2 based on cli.py\n    assert \"Error: For Terraform, either --tf-state-file must be provided.\" in result.stderr\n    result_not_found = run_iac_cli(tmp_path, [\"--iac-type\", \"terraform\", \"--tf-state-file\", \"nonexistent.tfstate\"])\n    # print(\"STDOUT:\", result_not_found.stdout)\n    # print(\"STDERR:\", result_not_found.stderr)",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_drift_detection_with_mock_connector",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def test_cli_drift_detection_with_mock_connector(temp_tfstate_file: Path, tmp_path: Path):\n    \"\"\"\n    Test end-to-end drift detection using a sample tfstate and the default mock connector.\n    \"\"\"\n    tfstate_file_abs_path = str(temp_tfstate_file.resolve())\n    args = [\n        \"--iac-type\", \"terraform\",\n        \"--tf-state-file\", tfstate_file_abs_path,\n        \"--actual-state-source\", \"mock\"\n    ]",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_no_drift_scenario",
        "kind": 2,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "def test_cli_no_drift_scenario(tmp_path: Path):\n    \"\"\"Test a scenario where desired and (mocked) actual states match.\"\"\"\n    # Create a tfstate file that perfectly matches a subset of the default mock data\n    # For simplicity, let's use only the S3 bucket part of the mock data, but make it match\n    no_drift_tfstate_content = {\n        \"version\": 4, \"resources\": [\n            {\n                \"mode\": \"managed\", \"type\": \"aws_s3_bucket\", \"name\": \"perfect_match_bucket\",\n                \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n                \"instances\": [{\"attributes\": {",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "CLI_MODULE_PATH",
        "kind": 5,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "CLI_MODULE_PATH = \"src.mcp_tools.iac_drift_detector.cli\"\n# Sample Terraform state content (from terraform_parser.py's example)\n# This represents the \"desired state\"\nSAMPLE_TFSTATE_CONTENT = {\n    \"version\": 4, \"terraform_version\": \"1.0.0\", \"serial\": 1, \"lineage\": \"some-uuid\", \"outputs\": {},\n    \"resources\": [\n        { # Will match mock actual, but with different attributes -> MODIFIED\n            \"mode\": \"managed\", \"type\": \"aws_instance\", \"name\": \"example_ec2\",\n            \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n            \"instances\": [{\"schema_version\": 1, \"attributes\": {",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "SAMPLE_TFSTATE_CONTENT",
        "kind": 5,
        "importPath": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "description": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "peekOfCode": "SAMPLE_TFSTATE_CONTENT = {\n    \"version\": 4, \"terraform_version\": \"1.0.0\", \"serial\": 1, \"lineage\": \"some-uuid\", \"outputs\": {},\n    \"resources\": [\n        { # Will match mock actual, but with different attributes -> MODIFIED\n            \"mode\": \"managed\", \"type\": \"aws_instance\", \"name\": \"example_ec2\",\n            \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",\n            \"instances\": [{\"schema_version\": 1, \"attributes\": {\n                \"id\": \"i-12345abcdef\", \"ami\": \"ami-0c55b31ad29f52962\",\n                \"instance_type\": \"t2.micro\", \"tags\": {\"Name\": \"example-instance\"}}}]\n        },",
        "detail": "tests.integration.test_iac_drift_detector.test_iac_drift_cli",
        "documentation": {}
    },
    {
        "label": "temp_git_repo",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def temp_git_repo(tmp_path: Path):\n    \"\"\"\n    Fixture to create a temporary Git repository for testing.\n    Yields the path to the repository.\n    \"\"\"\n    repo_dir = tmp_path / \"test_repo\"\n    repo_dir.mkdir()\n    # Initialize Git repo\n    repo = git.Repo.init(repo_dir)\n    # Initial commit on main branch",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def run_cli(repo_path: Path, args: list[str]) -> subprocess.CompletedProcess:\n    \"\"\"Helper function to run the CLI tool.\"\"\"\n    cmd = [\"python\", \"-m\", CLI_MODULE_PATH] + args\n    # print(f\"Running CLI: CWD={repo_path}, CMD={' '.join(cmd)}\")\n    return subprocess.run(cmd, capture_output=True, text=True, cwd=repo_path)\ndef create_policy_file(repo_path: Path, policy_content: dict):\n    \"\"\"Helper to create a .pr-policy.yml file in the repo.\"\"\"\n    with open(repo_path / DEFAULT_POLICY_FILENAME, 'w') as f:\n        yaml.dump(policy_content, f)\n# --- Basic CLI Tests ---",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "create_policy_file",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def create_policy_file(repo_path: Path, policy_content: dict):\n    \"\"\"Helper to create a .pr-policy.yml file in the repo.\"\"\"\n    with open(repo_path / DEFAULT_POLICY_FILENAME, 'w') as f:\n        yaml.dump(policy_content, f)\n# --- Basic CLI Tests ---\ndef test_cli_no_args_runs_with_defaults(temp_git_repo: Path):\n    \"\"\"Test running the CLI with no arguments in a simple repo. Expects defaults.\"\"\"\n    # Create a feature branch\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/good-branch\")",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_no_args_runs_with_defaults",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_no_args_runs_with_defaults(temp_git_repo: Path):\n    \"\"\"Test running the CLI with no arguments in a simple repo. Expects defaults.\"\"\"\n    # Create a feature branch\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/good-branch\")\n    (temp_git_repo / \"feature_file.txt\").write_text(\"Feature content\")\n    repo.index.add([\"feature_file.txt\"])\n    repo.index.commit(\"feat: add feature file\")\n    # Run CLI (should default to base=main, head=HEAD)\n    result = run_cli(temp_git_repo, [])",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_help_message",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_help_message():\n    \"\"\"Test if the CLI shows a help message.\"\"\"\n    # Run from anywhere, doesn't need a repo for --help\n    result = subprocess.run([\"python\", \"-m\", CLI_MODULE_PATH, \"--help\"], capture_output=True, text=True)\n    assert \"usage: cli.py\" in result.stdout # cli.py from argparse default prog name\n    assert \"--base-branch\" in result.stdout\n    assert result.returncode == 0\n# --- Policy Violation Tests ---\ndef test_cli_branch_name_violation(temp_git_repo: Path):\n    policy = {\"branch_naming\": {\"pattern\": \"^feature/.+$\", \"enabled\": True}}",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_branch_name_violation",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_branch_name_violation(temp_git_repo: Path):\n    policy = {\"branch_naming\": {\"pattern\": \"^feature/.+$\", \"enabled\": True}}\n    create_policy_file(temp_git_repo, policy)\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"badbranchname\") # Does not match \"feature/.+\"\n    (temp_git_repo / \"f.txt\").write_text(\"content\")\n    repo.index.add([\"f.txt\"])\n    repo.index.commit(\"feat: some commit\")\n    result = run_cli(temp_git_repo, [\"--base-branch\", \"main\", \"--head-branch\", \"badbranchname\"])\n    # print(\"STDOUT:\", result.stdout)",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_commit_message_conventional_type_violation",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_commit_message_conventional_type_violation(temp_git_repo: Path):\n    policy = {\"commit_messages\": {\"conventional_commit\": {\"types\": [\"feat\", \"fix\"], \"enabled\": True}, \"enabled\": True}}\n    create_policy_file(temp_git_repo, policy)\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/test-conv-commit\")\n    (temp_git_repo / \"f.txt\").write_text(\"content\")\n    repo.index.add([\"f.txt\"])\n    repo.index.commit(\"docs: this type is not allowed by policy\") # 'docs' not in types\n    result = run_cli(temp_git_repo, [\"--base-branch\", \"main\", \"--head-branch\", \"feature/test-conv-commit\"])\n    # print(\"STDOUT:\", result.stdout)",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_commit_message_missing_issue_violation",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_commit_message_missing_issue_violation(temp_git_repo: Path):\n    policy = {\n        \"commit_messages\": {\n            \"require_issue_number\": {\"pattern\": \"TASK-\\\\d+\", \"in_commit_body\": True, \"enabled\": True},\n            \"enabled\": True\n        }\n    }\n    create_policy_file(temp_git_repo, policy)\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/test-issue\")",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_disallowed_pattern_violation",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_disallowed_pattern_violation(temp_git_repo: Path):\n    policy = {\n        \"disallowed_patterns\": {\n            \"patterns\": [{\"pattern\": \"DO_NOT_COMMIT\", \"message\": \"Found forbidden string\", \"enabled\": True}],\n            \"enabled\": True\n        }\n    }\n    create_policy_file(temp_git_repo, policy)\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/disallowed\")",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_file_size_violation",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_file_size_violation(temp_git_repo: Path):\n    policy = {\"file_size\": {\"max_bytes\": 100, \"enabled\": True}} # Max 100 bytes\n    create_policy_file(temp_git_repo, policy)\n    repo = git.Repo(temp_git_repo)\n    repo.git.checkout(\"-b\", \"feature/large-file\")\n    # Create a file larger than 100 bytes\n    large_content = \"a\" * 200\n    (temp_git_repo / \"large_file.txt\").write_text(large_content)\n    repo.index.add([\"large_file.txt\"])\n    repo.index.commit(\"feat: add large file\")",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_multiple_violations",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_multiple_violations(temp_git_repo: Path):\n    policy = {\n        \"branch_naming\": {\"pattern\": \"^feature/.+$\", \"enabled\": True},\n        \"commit_messages\": {\n            \"conventional_commit\": {\"types\": [\"feat\"], \"enabled\": True},\n            \"enabled\": True\n        },\n        \"file_size\": {\"max_bytes\": 50, \"enabled\": True}\n    }\n    create_policy_file(temp_git_repo, policy)",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_no_new_commits",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_no_new_commits(temp_git_repo: Path):\n    \"\"\" Test behavior when there are no new commits between base and head.\"\"\"\n    create_policy_file(temp_git_repo, {}) # Default policy\n    repo = git.Repo(temp_git_repo)\n    # main branch is already HEAD here as it's the only commit\n    result = run_cli(temp_git_repo, [\"--base-branch\", \"main\", \"--head-branch\", \"main\"])\n    # print(\"STDOUT:\", result.stdout)\n    # print(\"STDERR:\", result.stderr)\n    assert \"No new commits found between main and main.\" in result.stdout\n    assert \"ALL CHECKS PASSED\" in result.stdout # No commits means no commit/file violations",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "test_cli_non_existent_config_file",
        "kind": 2,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "def test_cli_non_existent_config_file(temp_git_repo: Path):\n    result = run_cli(temp_git_repo, [\"--config-file\", \"non_existent.yml\"])\n    # print(\"STDOUT:\", result.stdout)\n    # print(\"STDERR:\", result.stderr)\n    assert result.returncode != 0 # Should be 3 based on cli.py\n    assert \"Configuration file not found: non_existent.yml\" in result.stderr # Error message to stderr\n# More tests could include:\n# - Invalid repo path\n# - Invalid base/head branch names\n# - Config file with invalid YAML structure",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "CLI_MODULE_PATH",
        "kind": 5,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "CLI_MODULE_PATH = \"src.mcp_tools.pr_reviewer.cli\"\nDEFAULT_POLICY_FILENAME = \".pr-policy.yml\"\n@pytest.fixture\ndef temp_git_repo(tmp_path: Path):\n    \"\"\"\n    Fixture to create a temporary Git repository for testing.\n    Yields the path to the repository.\n    \"\"\"\n    repo_dir = tmp_path / \"test_repo\"\n    repo_dir.mkdir()",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "DEFAULT_POLICY_FILENAME",
        "kind": 5,
        "importPath": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "description": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "peekOfCode": "DEFAULT_POLICY_FILENAME = \".pr-policy.yml\"\n@pytest.fixture\ndef temp_git_repo(tmp_path: Path):\n    \"\"\"\n    Fixture to create a temporary Git repository for testing.\n    Yields the path to the repository.\n    \"\"\"\n    repo_dir = tmp_path / \"test_repo\"\n    repo_dir.mkdir()\n    # Initialize Git repo",
        "detail": "tests.integration.test_pr_reviewer.test_pr_reviewer_cli",
        "documentation": {}
    },
    {
        "label": "clear_context_store_after_each_test",
        "kind": 2,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "def clear_context_store_after_each_test():\n    \"\"\"\n    Fixture to clear the CONTEXT_STORE after each test.\n    \"\"\"\n    CONTEXT_STORE.clear()\n    yield\n    CONTEXT_STORE.clear()\ndef test_echo_endpoint_direct():\n    \"\"\"\n    Test the /v1/tools/echo endpoint directly using TestClient.",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "test_echo_endpoint_direct",
        "kind": 2,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "def test_echo_endpoint_direct():\n    \"\"\"\n    Test the /v1/tools/echo endpoint directly using TestClient.\n    \"\"\"\n    test_message = \"Hello, !\" # \"Hello, direct!\" in Arabic\n    payload = {\"message\": test_message}\n    response = client.post(\"/v1/tools/echo\", json=payload)\n    assert response.status_code == 200\n    assert response.json() == {\"echoed_message\": test_message, \"context_id\": None}\ndef test_echo_endpoint_direct_with_context():",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "test_echo_endpoint_direct_with_context",
        "kind": 2,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "def test_echo_endpoint_direct_with_context():\n    \"\"\"\n    Test the /v1/tools/echo endpoint directly with a context_id.\n    \"\"\"\n    test_message = \"Hello, context!\"\n    context_id = \"test-context-123\"\n    # Optionally create the context first if the tool logic requires it\n    # client.post(\"/v1/contexts\", json={\"context_id\": context_id})\n    payload = {\"message\": test_message, \"context_id\": context_id}\n    response = client.post(\"/v1/tools/echo\", json=payload)",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "test_echo_tool_client_function",
        "kind": 2,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "def test_echo_tool_client_function(monkeypatch):\n    \"\"\"\n    Test the call_echo_tool client function against the TestClient.\n    This is an integration test for the client utility.\n    \"\"\"\n    test_message = \"Hello, client function!\"\n    # The client function expects a full URL. TestClient doesn't run a live server on a port by default.\n    # We can use the TestClient's base_url or mock httpx.post to direct to the TestClient.\n    # For a true integration test of the client against the app, it's simpler to use TestClient as the server.\n    # Method 1: Use TestClient as if it were a server (requires client to be adaptable or server to be live)",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "test_echo_tool_client_function_with_context",
        "kind": 2,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "def test_echo_tool_client_function_with_context(monkeypatch):\n    \"\"\"\n    Test the call_echo_tool client function with context_id against the TestClient.\n    \"\"\"\n    test_message = \"Client to context!\"\n    context_id = \"client-context-456\"\n    # client.post(f\"{client.base_url}/v1/contexts\", json={\"context_id\": context_id}) # Create context if necessary for tool\n    echo_response = call_echo_tool(\n        server_url=client.base_url,\n        message=test_message,",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "tests.integration.test_echo_tool",
        "description": "tests.integration.test_echo_tool",
        "peekOfCode": "client = TestClient(app)\n@pytest.fixture(autouse=True)\ndef clear_context_store_after_each_test():\n    \"\"\"\n    Fixture to clear the CONTEXT_STORE after each test.\n    \"\"\"\n    CONTEXT_STORE.clear()\n    yield\n    CONTEXT_STORE.clear()\ndef test_echo_endpoint_direct():",
        "detail": "tests.integration.test_echo_tool",
        "documentation": {}
    },
    {
        "label": "test_compare_attributes",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_attributes(iac_attrs, actual_attrs, resource_type, ignored_config, expected_drifts_count, expected_drift_details):\n    drifts = compare_attributes(iac_attrs, actual_attrs, resource_type, ignored_config)\n    assert len(drifts) == expected_drifts_count\n    for i, (attr_name, iac_val, act_val) in enumerate(expected_drift_details):\n        found_drift = next((d for d in drifts if d.attribute_name == attr_name), None)\n        assert found_drift is not None, f\"Expected drift for attribute '{attr_name}' not found.\"\n        assert found_drift.iac_value == iac_val\n        assert found_drift.actual_value == act_val\n# --- Test compare_states ---\ndef test_compare_states_modified_missing_unmanaged():",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_modified_missing_unmanaged",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_modified_missing_unmanaged():\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-instance-01\", type=\"vm\", name=\"web_server_iac\", provider_name=\"mock\", attributes={\"size\": \"medium\", \"image\": \"ubuntu-20.04\", \"tags\": {\"env\": \"prod\"}}),\n        ParsedResource(id=\"id-db-01\", type=\"database\", name=\"main_db_iac\", provider_name=\"mock\", attributes={\"version\": \"12\", \"storage\": \"100GB\"}), # Missing\n    ]\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-instance-01\", type=\"vm\", name=\"web_server_actual\", provider_name=\"mock\", attributes={\"size\": \"large\", \"image\": \"ubuntu-20.04\", \"tags\": {\"env\": \"prod\", \"extra_tag\": \"hello\"}}), # Modified size\n        ParsedResource(id=\"id-disk-unmanaged\", type=\"disk\", name=\"orphan_disk_actual\", provider_name=\"mock\", attributes={\"size\": \"50GB\"}), # Unmanaged\n    ]\n    drifts = compare_states(iac_state, actual_state)",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_no_drift",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_no_drift():\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-100\", type=\"vm\", name=\"app_server\", provider_name=\"mock\", attributes={\"image\": \"centos8\", \"cpu\": 2}),\n    ]\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-100\", type=\"vm\", name=\"app_server_live\", provider_name=\"mock\", attributes={\"image\": \"centos8\", \"cpu\": 2}),\n    ]\n    drifts = compare_states(iac_state, actual_state)\n    assert not drifts, f\"Expected no drift, but got: {drifts}\"\ndef test_compare_states_ignored_attributes():",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_ignored_attributes",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_ignored_attributes():\n    custom_ignored = {\"vm\": [\"last_updated_time\", \"dynamic_ip\"]}\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-200\", type=\"vm\", name=\"worker\", provider_name=\"mock\",\n                    attributes={\"image\": \"debian\", \"ram\": \"4GB\", \"last_updated_time\": \"ts1\", \"tags\": {\"managed_by\": \"iac\"}}),\n    ]\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-200\", type=\"vm\", name=\"worker_live\", provider_name=\"mock\",\n                       attributes={\"image\": \"debian\", \"ram\": \"4GB\", \"last_updated_time\": \"ts2\", \"dynamic_ip\": \"1.2.3.4\", \"tags\": {\"managed_by\": \"iac\", \"status\": \"running\"}}),\n    ]",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_tag_value_modified",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_tag_value_modified():\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-300\", type=\"vm\", name=\"tagged_vm\", provider_name=\"mock\",\n                    attributes={\"tags\": {\"env\": \"staging\", \"owner\": \"team-a\"}}),\n    ]\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-vm-300\", type=\"vm\", name=\"tagged_vm_live\", provider_name=\"mock\",\n                       attributes={\"tags\": {\"env\": \"prod\", \"owner\": \"team-a\", \"new_tag\": \"val\"}}),\n    ]\n    drifts = compare_states(iac_state, actual_state)",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_type_mismatch_on_same_id",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_type_mismatch_on_same_id():\n    \"\"\" Test scenario where same ID has different resource types (should be rare, but possible if IDs are not universally unique).\"\"\"\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-shared-01\", type=\"vm\", name=\"resource_a\", provider_name=\"mock\", attributes={})\n    ]\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-shared-01\", type=\"disk\", name=\"resource_b_actual\", provider_name=\"mock\", attributes={})\n    ]\n    drifts = compare_states(iac_state, actual_state)\n    assert len(drifts) == 1 # Should be one MODIFIED drift indicating type mismatch",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_empty_iac_all_unmanaged",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_empty_iac_all_unmanaged():\n    iac_state: List[ParsedResource] = []\n    actual_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-unmanaged-1\", type=\"vm\", name=\"vm1\", provider_name=\"mock\", attributes={}),\n        ParsedResource(id=\"id-unmanaged-2\", type=\"disk\", name=\"disk1\", provider_name=\"mock\", attributes={}),\n    ]\n    drifts = compare_states(iac_state, actual_state)\n    assert len(drifts) == 2\n    assert all(d.drift_type == DriftType.UNMANAGED_IN_ACTUAL for d in drifts)\ndef test_compare_states_empty_actual_all_missing():",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_empty_actual_all_missing",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_empty_actual_all_missing():\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=\"id-missing-1\", type=\"vm\", name=\"vm_iac_1\", provider_name=\"mock\", attributes={}),\n        ParsedResource(id=\"id-missing-2\", type=\"disk\", name=\"disk_iac_1\", provider_name=\"mock\", attributes={}),\n    ]\n    actual_state: List[ParsedResource] = []\n    drifts = compare_states(iac_state, actual_state)\n    assert len(drifts) == 2\n    assert all(d.drift_type == DriftType.MISSING_IN_ACTUAL for d in drifts)\ndef test_compare_states_no_ids_in_iac_resources():",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_compare_states_no_ids_in_iac_resources",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "description": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "peekOfCode": "def test_compare_states_no_ids_in_iac_resources():\n    \"\"\" Test how it handles IaC resources that might be missing 'id' (e.g. malformed state). \"\"\"\n    iac_state: List[ParsedResource] = [\n        ParsedResource(id=None, type=\"vm\", name=\"vm_no_id\", provider_name=\"mock\", attributes={}) # type: ignore\n    ]\n    actual_state: List[ParsedResource] = [\n         ParsedResource(id=\"id-actual-1\", type=\"vm\", name=\"vm_actual_1\", provider_name=\"mock\", attributes={})\n    ]\n    # Current implementation of compare_states uses `iac_by_id = {res.id: res for res in iac_resources if res.id}`\n    # So, iac_resource without id will be skipped for MODIFIED/MISSING checks based on ID.",
        "detail": "tests.unit.test_iac_drift_detector.test_drift_engine",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_missing_in_actual_terraform",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_missing_in_actual_terraform():\n    drift = DriftInfo(\n        drift_type=DriftType.MISSING_IN_ACTUAL,\n        resource_type=\"aws_instance\",\n        resource_name=\"web_server_01\",\n        iac_resource=ParsedResource(id=\"i-expected123\", type=\"aws_instance\", name=\"web_server_01\", provider_name=\"aws\", attributes={})\n    )\n    suggestions = suggest_remediation(drift, iac_tool=\"terraform\")\n    assert len(suggestions) >= 2\n    assert f\"Resource aws_instance.web_server_01 (Expected IaC ID: i-expected123) is defined in IaC but MISSING\" in suggestions[0]",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_missing_in_actual_with_module_terraform",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_missing_in_actual_with_module_terraform():\n    drift = DriftInfo(\n        drift_type=DriftType.MISSING_IN_ACTUAL,\n        resource_type=\"aws_instance\",\n        resource_name=\"web_server_module\",\n        iac_resource=ParsedResource(id=\"i-expected456\", type=\"aws_instance\", name=\"web_server_module\", provider_name=\"aws\", module=\"module.my_module\", attributes={})\n    )\n    suggestions = suggest_remediation(drift, iac_tool=\"terraform\")\n    assert len(suggestions) >= 3\n    assert \"(Resource is in module: module.my_module)\" in suggestions[2]",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_unmanaged_in_actual_terraform",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_unmanaged_in_actual_terraform():\n    drift = DriftInfo(\n        drift_type=DriftType.UNMANAGED_IN_ACTUAL,\n        resource_type=\"aws_s3_bucket\",\n        resource_name=\"manual-bucket-007\",\n        resource_id=\"manual-bucket-007-id\",\n        actual_resource=ParsedResource(id=\"manual-bucket-007-id\", type=\"aws_s3_bucket\", name=\"manual-bucket-007\", provider_name=\"aws\", attributes={})\n    )\n    suggestions = suggest_remediation(drift, iac_tool=\"terraform\")\n    assert len(suggestions) >= 4 # Main message + 3 suggestions",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_modified_terraform",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_modified_terraform():\n    attr_drifts = [\n        AttributeDrift(attribute_name=\"instance_type\", iac_value=\"t2.micro\", actual_value=\"t3.small\"),\n        AttributeDrift(attribute_name=\"monitoring\", iac_value=None, actual_value=True)\n    ]\n    drift = DriftInfo(\n        drift_type=DriftType.MODIFIED,\n        resource_type=\"aws_instance\",\n        resource_name=\"app_server_main\",\n        resource_id=\"i-actual456\",",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_unknown_drift_type",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_unknown_drift_type():\n    # Create a dummy DriftType or mock it if Enum doesn't allow easy extension for tests\n    class MockUnknownDriftType:\n        value = \"very_unknown_drift\"\n    drift = DriftInfo(\n        drift_type=MockUnknownDriftType(), # type: ignore\n        resource_type=\"some_type\",\n        resource_name=\"some_name\",\n        resource_id=\"some_id\"\n    )",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "test_suggest_remediation_generic_iac_tool",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_remediation",
        "description": "tests.unit.test_iac_drift_detector.test_remediation",
        "peekOfCode": "def test_suggest_remediation_generic_iac_tool():\n    drift_missing = DriftInfo(\n        drift_type=DriftType.MISSING_IN_ACTUAL,\n        resource_type=\"generic_resource\",\n        resource_name=\"test_res\",\n        iac_resource=ParsedResource(id=\"gen-id-123\", type=\"generic_resource\", name=\"test_res\", provider_name=\"any\", attributes={})\n    )\n    suggestions = suggest_remediation(drift_missing, iac_tool=\"pulumi\") # Example other tool\n    assert len(suggestions) >= 2\n    assert \"Use your IaC tool to apply the configuration and create the resource.\" in suggestions[1]",
        "detail": "tests.unit.test_iac_drift_detector.test_remediation",
        "documentation": {}
    },
    {
        "label": "sample_tfstate_content",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def sample_tfstate_content() -> dict:\n    return {\n        \"version\": 4,\n        \"terraform_version\": \"1.1.0\",\n        \"resources\": [\n            {\n                \"mode\": \"managed\",\n                \"type\": \"aws_instance\",\n                \"name\": \"web_server\",\n                \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "temp_tfstate_file",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def temp_tfstate_file(tmp_path: Path, sample_tfstate_content: dict) -> Path:\n    file_path = tmp_path / \"test.tfstate\"\n    with open(file_path, 'w') as f:\n        json.dump(sample_tfstate_content, f)\n    return file_path\n@pytest.fixture\ndef sample_tfplan_json_content() -> dict:\n    return {\n        \"format_version\": \"1.0\",\n        \"resource_changes\": [",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "sample_tfplan_json_content",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def sample_tfplan_json_content() -> dict:\n    return {\n        \"format_version\": \"1.0\",\n        \"resource_changes\": [\n            {\n                \"address\": \"aws_instance.web_server_new\",\n                \"type\": \"aws_instance\", \"name\": \"web_server_new\",\n                \"change\": {\"actions\": [\"create\"], \"after\": {\"instance_type\": \"t3.micro\"}}\n            },\n            {",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "temp_tfplan_json_file",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def temp_tfplan_json_file(tmp_path: Path, sample_tfplan_json_content: dict) -> Path:\n    file_path = tmp_path / \"test_plan.json\"\n    with open(file_path, 'w') as f:\n        json.dump(sample_tfplan_json_content, f)\n    return file_path\n# --- Tests for parse_terraform_state_file ---\ndef test_parse_tfstate_valid_file(temp_tfstate_file: Path):\n    resources = parse_terraform_state_file(str(temp_tfstate_file))\n    assert len(resources) == 3 # 2 managed resources + 1 null_resource, data source ignored\n    instance_res = next((r for r in resources if r.type == \"aws_instance\"), None)",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_valid_file",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_valid_file(temp_tfstate_file: Path):\n    resources = parse_terraform_state_file(str(temp_tfstate_file))\n    assert len(resources) == 3 # 2 managed resources + 1 null_resource, data source ignored\n    instance_res = next((r for r in resources if r.type == \"aws_instance\"), None)\n    assert instance_res is not None\n    assert instance_res.id == \"i-0123456789abcdef0\"\n    assert instance_res.name == \"web_server\"\n    assert instance_res.provider_name == \"aws\"\n    assert instance_res.attributes[\"instance_type\"] == \"t2.micro\"\n    assert instance_res.attributes[\"tags\"] == {\"Name\": \"web-server-prod\"}",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_empty_resources",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_empty_resources():\n    empty_tfstate = {\"version\": 4, \"resources\": []}\n    # Use Path object from tmp_path for writing\n    file_path = Path(Path(__file__).parent / \"empty.tfstate\") # Not ideal, should use tmp_path\n    with open(file_path, 'w') as f:\n        json.dump(empty_tfstate, f)\n    resources = parse_terraform_state_file(str(file_path))\n    assert len(resources) == 0\n    file_path.unlink() # Clean up\ndef test_parse_tfstate_no_resources_key(tmp_path: Path):",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_no_resources_key",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_no_resources_key(tmp_path: Path):\n    no_res_key_tfstate = {\"version\": 4} # Missing 'resources' key\n    file_path = tmp_path / \"no_res.tfstate\"\n    with open(file_path, 'w') as f:\n        json.dump(no_res_key_tfstate, f)\n    resources = parse_terraform_state_file(str(file_path))\n    assert len(resources) == 0\ndef test_parse_tfstate_file_not_found(capsys):\n    resources = parse_terraform_state_file(\"non_existent_file.tfstate\")\n    assert len(resources) == 0",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_file_not_found",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_file_not_found(capsys):\n    resources = parse_terraform_state_file(\"non_existent_file.tfstate\")\n    assert len(resources) == 0\n    captured = capsys.readouterr()\n    assert \"Error: Terraform state file not found\" in captured.err\ndef test_parse_tfstate_invalid_json(tmp_path: Path, capsys):\n    file_path = tmp_path / \"invalid.tfstate\"\n    file_path.write_text(\"this is not json\")\n    resources = parse_terraform_state_file(str(file_path))\n    assert len(resources) == 0",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_invalid_json",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_invalid_json(tmp_path: Path, capsys):\n    file_path = tmp_path / \"invalid.tfstate\"\n    file_path.write_text(\"this is not json\")\n    resources = parse_terraform_state_file(str(file_path))\n    assert len(resources) == 0\n    captured = capsys.readouterr()\n    assert \"Error: Invalid JSON\" in captured.err\n# --- Tests for parse_terraform_plan_json_file ---\ndef test_parse_tfplan_valid_file(temp_tfplan_json_file: Path):\n    changes = parse_terraform_plan_json_file(str(temp_tfplan_json_file))",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfplan_valid_file",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfplan_valid_file(temp_tfplan_json_file: Path):\n    changes = parse_terraform_plan_json_file(str(temp_tfplan_json_file))\n    assert len(changes) == 3 # Includes no-op for now\n    create_change = next((c for c in changes if c[\"change\"][\"actions\"] == [\"create\"]), None)\n    assert create_change is not None\n    assert create_change[\"address\"] == \"aws_instance.web_server_new\"\n    assert create_change[\"change\"][\"after\"][\"instance_type\"] == \"t3.micro\"\n    update_change = next((c for c in changes if c[\"change\"][\"actions\"] == [\"update\"]), None)\n    assert update_change is not None\n    assert update_change[\"address\"] == \"aws_s3_bucket.my_data_bucket\"",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfplan_file_not_found",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfplan_file_not_found(capsys):\n    changes = parse_terraform_plan_json_file(\"non_existent_plan.json\")\n    assert len(changes) == 0\n    captured = capsys.readouterr()\n    assert \"Error: Terraform plan JSON file not found\" in captured.err\ndef test_parse_tfplan_invalid_json(tmp_path: Path, capsys):\n    file_path = tmp_path / \"invalid_plan.json\"\n    file_path.write_text(\"{not_json_at_all\")\n    changes = parse_terraform_plan_json_file(str(file_path))\n    assert len(changes) == 0",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfplan_invalid_json",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfplan_invalid_json(tmp_path: Path, capsys):\n    file_path = tmp_path / \"invalid_plan.json\"\n    file_path.write_text(\"{not_json_at_all\")\n    changes = parse_terraform_plan_json_file(str(file_path))\n    assert len(changes) == 0\n    captured = capsys.readouterr()\n    assert \"Error: Invalid JSON\" in captured.err\ndef test_parse_tfplan_empty_changes(tmp_path: Path):\n    plan_content = {\"format_version\": \"1.0\", \"resource_changes\": []}\n    file_path = tmp_path / \"empty_changes_plan.json\"",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfplan_empty_changes",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfplan_empty_changes(tmp_path: Path):\n    plan_content = {\"format_version\": \"1.0\", \"resource_changes\": []}\n    file_path = tmp_path / \"empty_changes_plan.json\"\n    with open(file_path, 'w') as f:\n        json.dump(plan_content, f)\n    changes = parse_terraform_plan_json_file(str(file_path))\n    assert len(changes) == 0\ndef test_parse_tfstate_resource_missing_id_type_name(tmp_path: Path, capsys):\n    # Test case where a resource instance might be malformed (e.g., missing 'id')\n    malformed_tfstate_content = {",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "test_parse_tfstate_resource_missing_id_type_name",
        "kind": 2,
        "importPath": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "description": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "peekOfCode": "def test_parse_tfstate_resource_missing_id_type_name(tmp_path: Path, capsys):\n    # Test case where a resource instance might be malformed (e.g., missing 'id')\n    malformed_tfstate_content = {\n        \"version\": 4,\n        \"resources\": [\n            {\n                \"mode\": \"managed\",\n                \"type\": \"aws_instance\",\n                \"name\": \"bad_instance\",\n                \"provider\": \"provider[\\\"registry.terraform.io/hashicorp/aws\\\"]\",",
        "detail": "tests.unit.test_iac_drift_detector.test_terraform_parser",
        "documentation": {}
    },
    {
        "label": "temp_config_file",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def temp_config_file(tmp_path):\n    \"\"\"Fixture to create a temporary config file and clean it up.\"\"\"\n    file_path = tmp_path / DEFAULT_CONFIG_FILENAME\n    def _create_config(content_dict):\n        with open(file_path, 'w') as f:\n            yaml.dump(content_dict, f)\n        return file_path\n    yield _create_config\n    # No explicit cleanup needed for tmp_path, pytest handles it\ndef test_load_config_default_values():",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_default_values",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_default_values():\n    \"\"\"Test loading config when no file exists, should return defaults.\"\"\"\n    config = load_config(config_path=\"non_existent_file.yml\") # Should trigger default loading path if not found\n    assert isinstance(config, PolicyConfig)\n    assert config.branch_naming.enabled is True\n    assert config.branch_naming.pattern.pattern == \"^(feature|fix|chore|docs|style|refactor|test)/[a-zA-Z0-9_.-]+$\"\n    assert config.commit_messages.enabled is True\n    assert config.file_size.max_bytes == 1048576\ndef test_load_config_from_file(temp_config_file):\n    \"\"\"Test loading a valid configuration from a YAML file.\"\"\"",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_from_file",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_from_file(temp_config_file):\n    \"\"\"Test loading a valid configuration from a YAML file.\"\"\"\n    custom_config_data = {\n        \"branch_naming\": {\"pattern\": \"^custom/.+$\", \"enabled\": True},\n        \"commit_messages\": {\n            \"conventional_commit\": {\"types\": [\"task\", \"bugfix\"]},\n            \"require_issue_number\": {\"enabled\": True, \"pattern\": \"TASK-\\\\d+\"},\n            \"enabled\": True\n        },\n        \"disallowed_patterns\": {",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_empty_file",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_empty_file(temp_config_file):\n    \"\"\"Test loading an empty YAML file, should use defaults.\"\"\"\n    config_file_path = temp_config_file({}) # Empty dict makes an empty YAML file\n    original_cwd = os.getcwd()\n    os.chdir(config_file_path.parent)\n    try:\n        config = load_config()\n    finally:\n        os.chdir(original_cwd)\n    assert isinstance(config, PolicyConfig)",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_partial_config",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_partial_config(temp_config_file):\n    \"\"\"Test loading a file with only some sections defined.\"\"\"\n    partial_data = {\n        \"branch_naming\": {\"enabled\": False}\n    }\n    config_file_path = temp_config_file(partial_data)\n    original_cwd = os.getcwd()\n    os.chdir(config_file_path.parent)\n    try:\n        config = load_config()",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_invalid_yaml",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_invalid_yaml(temp_config_file):\n    \"\"\"Test loading a file with invalid YAML content.\"\"\"\n    file_path = temp_config_file(None) # Create empty file first\n    with open(file_path, 'w') as f:\n        f.write(\"branch_naming: {pattern: 'foo', enabled: true\") # Missing closing }\n    original_cwd = os.getcwd()\n    os.chdir(file_path.parent)\n    with pytest.raises(ValueError, match=\"Error parsing YAML configuration file\"):\n        try:\n            load_config()",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_load_config_validation_error",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_load_config_validation_error(temp_config_file):\n    \"\"\"Test loading a file with valid YAML but data that fails Pydantic validation.\"\"\"\n    invalid_data = {\n        \"file_size\": {\"max_bytes\": \"not-an-integer\"}\n    }\n    config_file_path = temp_config_file(invalid_data)\n    original_cwd = os.getcwd()\n    os.chdir(config_file_path.parent)\n    with pytest.raises(ValueError, match=\"Configuration validation error\"):\n        try:",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_regex_compilation_in_models",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_regex_compilation_in_models():\n    \"\"\"Test that regex patterns are compiled correctly in Pydantic models.\"\"\"\n    bn_policy = BranchNamingPolicy(pattern=\"^test/.+$\")\n    assert isinstance(bn_policy.pattern, re.Pattern)\n    assert bn_policy.pattern.pattern == \"^test/.+$\"\n    ri_policy = RequireIssueNumberPolicy(pattern=\"^T-\\\\d+$\", enabled=True)\n    assert isinstance(ri_policy.pattern, re.Pattern)\n    assert ri_policy.pattern.pattern == \"^T-\\\\d+$\"\n    dp_item = DisallowedPatternItem(pattern=\"secret\")\n    assert isinstance(dp_item.pattern, re.Pattern)",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_invalid_regex_pattern_in_models",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_invalid_regex_pattern_in_models():\n    \"\"\"Test that invalid regex patterns raise ValueError during model instantiation.\"\"\"\n    with pytest.raises(ValidationError): # Pydantic wraps it in ValidationError\n        BranchNamingPolicy(pattern=\"*invalidregex\")\n    with pytest.raises(ValidationError):\n        RequireIssueNumberPolicy(pattern=\"[\", enabled=True)\n    with pytest.raises(ValidationError):\n        DisallowedPatternItem(pattern=\"(?<invalid)\")\ndef test_default_config_file_search_logic(tmp_path):\n    \"\"\"Test the search logic for the default config file.\"\"\"",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_default_config_file_search_logic",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_default_config_file_search_logic(tmp_path):\n    \"\"\"Test the search logic for the default config file.\"\"\"\n    # Setup: create a .pr-policy.yml in a subdirectory\n    project_root = tmp_path\n    sub_dir = project_root / \"subdir1\" / \"subdir2\"\n    sub_dir.mkdir(parents=True)\n    config_content = {\"branch_naming\": {\"pattern\": \"^search_logic_test/.+$\"}}\n    with open(sub_dir / DEFAULT_CONFIG_FILENAME, 'w') as f:\n        yaml.dump(config_content, f)\n    # Test 1: Run from a deeper directory, should find the file in parent",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_branch_naming_policy_defaults",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_branch_naming_policy_defaults():\n    policy = BranchNamingPolicy()\n    assert policy.enabled is True\n    assert policy.pattern.pattern == \"^(feature|fix|chore|docs|style|refactor|test)/[a-zA-Z0-9_.-]+$\"\ndef test_conventional_commit_policy_defaults():\n    policy = ConventionalCommitPolicy()\n    assert policy.enabled is True\n    assert policy.types == [\"feat\", \"fix\", \"docs\", \"style\", \"refactor\", \"test\", \"chore\"]\ndef test_require_issue_number_policy_defaults():\n    policy = RequireIssueNumberPolicy()",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_conventional_commit_policy_defaults",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_conventional_commit_policy_defaults():\n    policy = ConventionalCommitPolicy()\n    assert policy.enabled is True\n    assert policy.types == [\"feat\", \"fix\", \"docs\", \"style\", \"refactor\", \"test\", \"chore\"]\ndef test_require_issue_number_policy_defaults():\n    policy = RequireIssueNumberPolicy()\n    assert policy.enabled is False\n    assert policy.pattern.pattern == \"\\\\[[A-Z]+-[0-9]+\\\\]\"\n    assert policy.in_commit_body is True\ndef test_disallowed_patterns_policy_defaults():",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_require_issue_number_policy_defaults",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_require_issue_number_policy_defaults():\n    policy = RequireIssueNumberPolicy()\n    assert policy.enabled is False\n    assert policy.pattern.pattern == \"\\\\[[A-Z]+-[0-9]+\\\\]\"\n    assert policy.in_commit_body is True\ndef test_disallowed_patterns_policy_defaults():\n    policy = DisallowedPatternsPolicy()\n    assert policy.enabled is True\n    assert policy.patterns == []\ndef test_file_size_policy_defaults():",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_disallowed_patterns_policy_defaults",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_disallowed_patterns_policy_defaults():\n    policy = DisallowedPatternsPolicy()\n    assert policy.enabled is True\n    assert policy.patterns == []\ndef test_file_size_policy_defaults():\n    policy = FileSizePolicy()\n    assert policy.enabled is True\n    assert policy.max_bytes == 1048576\n    assert policy.ignore_extensions == []\n    assert policy.ignore_paths == []",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_file_size_policy_defaults",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_config",
        "description": "tests.unit.test_pr_reviewer.test_config",
        "peekOfCode": "def test_file_size_policy_defaults():\n    policy = FileSizePolicy()\n    assert policy.enabled is True\n    assert policy.max_bytes == 1048576\n    assert policy.ignore_extensions == []\n    assert policy.ignore_paths == []",
        "detail": "tests.unit.test_pr_reviewer.test_config",
        "documentation": {}
    },
    {
        "label": "test_check_branch_name_policy_valid",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_branch_name_policy_valid():\n    policy = BranchNamingPolicy(pattern=\"^(feat|fix)/[a-z0-9-]+$\", enabled=True)\n    assert branch_policies.check_branch_name_policy(\"feat/new-stuff-123\", policy) == []\ndef test_check_branch_name_policy_invalid():\n    pattern_str = \"^(feat|fix)/[a-z0-9-]+$\"\n    policy = BranchNamingPolicy(pattern=pattern_str, enabled=True)\n    violations = branch_policies.check_branch_name_policy(\"Feature/InvalidName\", policy)\n    assert len(violations) == 1\n    assert f\"does not match the required pattern: '{pattern_str}'\" in violations[0]\ndef test_check_branch_name_policy_disabled():",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_branch_name_policy_invalid",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_branch_name_policy_invalid():\n    pattern_str = \"^(feat|fix)/[a-z0-9-]+$\"\n    policy = BranchNamingPolicy(pattern=pattern_str, enabled=True)\n    violations = branch_policies.check_branch_name_policy(\"Feature/InvalidName\", policy)\n    assert len(violations) == 1\n    assert f\"does not match the required pattern: '{pattern_str}'\" in violations[0]\ndef test_check_branch_name_policy_disabled():\n    policy = BranchNamingPolicy(pattern=\"^valid/.+$\", enabled=False)\n    assert branch_policies.check_branch_name_policy(\"anything/goes\", policy) == []\ndef test_check_branch_name_policy_detached_head():",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_branch_name_policy_disabled",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_branch_name_policy_disabled():\n    policy = BranchNamingPolicy(pattern=\"^valid/.+$\", enabled=False)\n    assert branch_policies.check_branch_name_policy(\"anything/goes\", policy) == []\ndef test_check_branch_name_policy_detached_head():\n    policy = BranchNamingPolicy(pattern=\"^valid/.+$\", enabled=True)\n    violations = branch_policies.check_branch_name_policy(None, policy)\n    assert len(violations) == 1\n    assert \"Branch name could not be determined\" in violations[0]\n# --- Tests for commit policies ---\n@pytest.mark.parametrize(\"subject, types, is_valid\", [",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_branch_name_policy_detached_head",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_branch_name_policy_detached_head():\n    policy = BranchNamingPolicy(pattern=\"^valid/.+$\", enabled=True)\n    violations = branch_policies.check_branch_name_policy(None, policy)\n    assert len(violations) == 1\n    assert \"Branch name could not be determined\" in violations[0]\n# --- Tests for commit policies ---\n@pytest.mark.parametrize(\"subject, types, is_valid\", [\n    (\"feat: add new feature\", [\"feat\", \"fix\"], True),\n    (\"fix(scope): resolve bug\", [\"feat\", \"fix\"], True),\n    (\"docs!: update README with breaking change\", [\"docs\", \"feat\"], True),",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_conventional_commit_format",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_conventional_commit_format(subject, types, is_valid):\n    policy = ConventionalCommitPolicy(enabled=True, types=types)\n    violations = commit_policies.check_conventional_commit_format(subject, \"sha123\", policy)\n    if is_valid:\n        assert not violations, f\"Expected no violations for '{subject}' with types {types}\"\n    else:\n        assert violations, f\"Expected violations for '{subject}' with types {types}\"\ndef test_check_conventional_commit_format_disabled():\n    policy = ConventionalCommitPolicy(enabled=False, types=[\"feat\"])\n    violations = commit_policies.check_conventional_commit_format(\"anything goes\", \"sha123\", policy)",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_conventional_commit_format_disabled",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_conventional_commit_format_disabled():\n    policy = ConventionalCommitPolicy(enabled=False, types=[\"feat\"])\n    violations = commit_policies.check_conventional_commit_format(\"anything goes\", \"sha123\", policy)\n    assert not violations\n@pytest.mark.parametrize(\"body, pattern_str, pr_title, pr_body, in_commit_body, expected_violations_count\", [\n    (\"Fixes TICKET-123\", r\"TICKET-\\d+\", None, None, True, 0),\n    (\"Related to task [PROJ-001]\", r\"\\[PROJ-\\d+\\]\", None, None, True, 0),\n    (\"No ticket here.\", r\"TICKET-\\d+\", None, None, True, 1),\n    (\"Body has TICKET-123\", r\"TICKET-\\d+\", None, None, False, 0), # Policy check for body disabled\n    # Future tests for PR title/body would go here",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_commit_for_issue_number",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_commit_for_issue_number(body, pattern_str, pr_title, pr_body, in_commit_body, expected_violations_count):\n    policy = RequireIssueNumberPolicy(pattern=pattern_str, in_commit_body=in_commit_body, enabled=True)\n    violations = commit_policies.check_commit_for_issue_number(body, pr_title, pr_body, \"sha123\", policy)\n    assert len(violations) == expected_violations_count\ndef test_check_commit_for_issue_number_disabled():\n    policy = RequireIssueNumberPolicy(pattern=r\"TICKET-\\d+\", enabled=False)\n    violations = commit_policies.check_commit_for_issue_number(\"No ticket needed\", None, None, \"sha123\", policy)\n    assert not violations\ndef test_check_commit_message_policies_orchestration():\n    commit_details = {",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_commit_for_issue_number_disabled",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_commit_for_issue_number_disabled():\n    policy = RequireIssueNumberPolicy(pattern=r\"TICKET-\\d+\", enabled=False)\n    violations = commit_policies.check_commit_for_issue_number(\"No ticket needed\", None, None, \"sha123\", policy)\n    assert not violations\ndef test_check_commit_message_policies_orchestration():\n    commit_details = {\n        \"sha\": \"testsha\",\n        \"message_subject\": \"badtype: this is a test\",\n        \"message_body\": \"This commit has no ticket reference.\"\n    }",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_commit_message_policies_orchestration",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_commit_message_policies_orchestration():\n    commit_details = {\n        \"sha\": \"testsha\",\n        \"message_subject\": \"badtype: this is a test\",\n        \"message_body\": \"This commit has no ticket reference.\"\n    }\n    policy = CommitMessagePolicy(\n        conventional_commit=ConventionalCommitPolicy(enabled=True, types=[\"feat\", \"fix\"]),\n        require_issue_number=RequireIssueNumberPolicy(pattern=r\"TICKET-\\d+\", in_commit_body=True, enabled=True),\n        enabled=True",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "mock_get_file_content",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def mock_get_file_content(filepath: str) -> Optional[AnyStr]:\n    return mock_file_contents.get(filepath)\n@pytest.mark.parametrize(\"filepath, patterns_config, expected_violations_count, expected_messages_contain\", [\n    (\"secrets.py\", [DisallowedPatternItem(pattern=\"API_KEY\\\\s*=\", enabled=True)], 1, [\"API_KEY\"]),\n    (\"secrets.py\", [DisallowedPatternItem(pattern=\"PASSWORD\\\\s*=\", enabled=True)], 1, [\"PASSWORD\"]),\n    (\"secrets.py\", [\n        DisallowedPatternItem(pattern=\"API_KEY\\\\s*=\", enabled=True),\n        DisallowedPatternItem(pattern=\"PASSWORD\\\\s*=\", enabled=True)\n    ], 2, [\"API_KEY\", \"PASSWORD\"]),\n    (\"clean.txt\", [DisallowedPatternItem(pattern=\"SECRET\", enabled=True)], 0, []),",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_content_disallowed_patterns",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_content_disallowed_patterns(filepath, patterns_config, expected_violations_count, expected_messages_contain):\n    policy = DisallowedPatternsPolicy(patterns=patterns_config, enabled=True)\n    violations = file_policies.check_content_disallowed_patterns(filepath, mock_get_file_content, policy)\n    assert len(violations) == expected_violations_count\n    for msg_part in expected_messages_contain:\n        assert any(msg_part in v for v in violations), f\"Expected part '{msg_part}' not in violations: {violations}\"\ndef test_check_content_disallowed_patterns_disabled():\n    policy = DisallowedPatternsPolicy(patterns=[DisallowedPatternItem(pattern=\"SECRET\", enabled=True)], enabled=False)\n    violations = file_policies.check_content_disallowed_patterns(\"secrets.py\", mock_get_file_content, policy)\n    assert not violations",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_content_disallowed_patterns_disabled",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_content_disallowed_patterns_disabled():\n    policy = DisallowedPatternsPolicy(patterns=[DisallowedPatternItem(pattern=\"SECRET\", enabled=True)], enabled=False)\n    violations = file_policies.check_content_disallowed_patterns(\"secrets.py\", mock_get_file_content, policy)\n    assert not violations\n# Mock file size getter for size tests\nmock_file_sizes = {\n    \"small.txt\": 100,\n    \"large.exe\": 2000000,\n    \"ignored.log\": 5000000,\n    \"vendor/big_lib.js\": 3000000,",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "mock_get_file_size",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def mock_get_file_size(filepath: str) -> Optional[int]:\n    return mock_file_sizes.get(filepath)\n@pytest.mark.parametrize(\"filepath, max_bytes, ignore_ext, ignore_paths, expected_violations_count\", [\n    (\"small.txt\", 1000, [], [], 0),\n    (\"large.exe\", 1000000, [], [], 1), # Exceeds 1MB\n    (\"ignored.log\", 100, [\".log\"], [], 0), # Ignored by extension\n    (\"vendor/big_lib.js\", 1000, [], [\"vendor/*\"], 0), # Ignored by path\n    (\"docs/image.png\", 1000, [], [], 1), # Exceeds 1000 bytes (not 1KB)\n    (\"not_found.txt\", 1000, [], [], 0), # Size unknown, skipped\n])",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_file_size_policy",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_file_size_policy(filepath, max_bytes, ignore_ext, ignore_paths, expected_violations_count):\n    policy = FileSizePolicy(max_bytes=max_bytes, ignore_extensions=ignore_ext, ignore_paths=ignore_paths, enabled=True)\n    violations = file_policies.check_file_size_policy(filepath, mock_get_file_size, policy)\n    assert len(violations) == expected_violations_count\ndef test_check_file_size_policy_disabled():\n    policy = FileSizePolicy(max_bytes=10, enabled=False)\n    violations = file_policies.check_file_size_policy(\"large.exe\", mock_get_file_size, policy)\n    assert not violations",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_check_file_size_policy_disabled",
        "kind": 2,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "def test_check_file_size_policy_disabled():\n    policy = FileSizePolicy(max_bytes=10, enabled=False)\n    violations = file_policies.check_file_size_policy(\"large.exe\", mock_get_file_size, policy)\n    assert not violations",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "mock_file_contents",
        "kind": 5,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "mock_file_contents = {\n    \"secrets.py\": \"API_KEY = '12345'\\nPASSWORD = \\\"secret\\\"\\nOTHER_VAR='ok'\",\n    \"clean.txt\": \"This file is clean.\",\n    \"binary.data\": b\"\\x00\\x01\\x02SECRET_KEY\", # Will be skipped by content check\n    \"utf8_error.txt\": b\"Invalid \\xff UTF-8\" # Will be skipped\n}\ndef mock_get_file_content(filepath: str) -> Optional[AnyStr]:\n    return mock_file_contents.get(filepath)\n@pytest.mark.parametrize(\"filepath, patterns_config, expected_violations_count, expected_messages_contain\", [\n    (\"secrets.py\", [DisallowedPatternItem(pattern=\"API_KEY\\\\s*=\", enabled=True)], 1, [\"API_KEY\"]),",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "mock_file_sizes",
        "kind": 5,
        "importPath": "tests.unit.test_pr_reviewer.test_policies",
        "description": "tests.unit.test_pr_reviewer.test_policies",
        "peekOfCode": "mock_file_sizes = {\n    \"small.txt\": 100,\n    \"large.exe\": 2000000,\n    \"ignored.log\": 5000000,\n    \"vendor/big_lib.js\": 3000000,\n    \"docs/image.png\": 1024, # 1KB\n    \"not_found.txt\": None\n}\ndef mock_get_file_size(filepath: str) -> Optional[int]:\n    return mock_file_sizes.get(filepath)",
        "detail": "tests.unit.test_pr_reviewer.test_policies",
        "documentation": {}
    },
    {
        "label": "test_health_check",
        "kind": 2,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "def test_health_check():\n    \"\"\"\n    Test the /health endpoint.\n    It should return a 200 OK status and a JSON response with {\"status\": \"ok\"}.\n    \"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"ok\"}\ndef test_create_context_success():\n    \"\"\"",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "test_create_context_success",
        "kind": 2,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "def test_create_context_success():\n    \"\"\"\n    Test successful context creation.\n    \"\"\"\n    context_id = \"test_context_01\"\n    response = client.post(\"/v1/contexts\", json={\"context_id\": context_id})\n    assert response.status_code == 201\n    assert response.json() == {\"message\": \"Context created successfully\", \"context_id\": context_id}\n    # Verify it's in the store (optional, depends on how much you want to test internal state)\n    # This requires access to CONTEXT_STORE or a way to inspect it.",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "test_create_context_conflict",
        "kind": 2,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "def test_create_context_conflict():\n    \"\"\"\n    Test creating a context that already exists.\n    \"\"\"\n    context_id = \"test_context_conflict\"\n    # Create it once\n    client.post(\"/v1/contexts\", json={\"context_id\": context_id})\n    # Try to create it again\n    response = client.post(\"/v1/contexts\", json={\"context_id\": context_id})\n    assert response.status_code == 409",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "test_get_context_not_found",
        "kind": 2,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "def test_get_context_not_found():\n    \"\"\"\n    Test retrieving a context that does not exist.\n    \"\"\"\n    response = client.get(\"/v1/contexts/non_existent_context\")\n    assert response.status_code == 404\n    assert response.json() == {\"detail\": \"Context not found\"}\n# Clean up contexts created during tests to ensure test isolation if needed\n# For simple in-memory store, this might not be strictly necessary if TestClient re-initializes state,\n# but good practice for more complex scenarios.",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "clear_context_store_after_each_test",
        "kind": 2,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "def clear_context_store_after_each_test():\n    \"\"\"\n    Fixture to clear the CONTEXT_STORE after each test that might modify it.\n    \"\"\"\n    # Setup:\n    # Could backup CONTEXT_STORE here if needed\n    yield # This is where the test runs\n    # Teardown: Clear the CONTEXT_STORE\n    from src.mcp_server.main import CONTEXT_STORE\n    CONTEXT_STORE.clear()",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "tests.unit.test_server",
        "description": "tests.unit.test_server",
        "peekOfCode": "client = TestClient(app)\ndef test_health_check():\n    \"\"\"\n    Test the /health endpoint.\n    It should return a 200 OK status and a JSON response with {\"status\": \"ok\"}.\n    \"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"ok\"}\ndef test_create_context_success():",
        "detail": "tests.unit.test_server",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    print(\"Hello from app!\")\nif __name__ == \"__main__\":\n    main()",
        "detail": "main",
        "documentation": {}
    }
]